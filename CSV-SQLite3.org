# -*- mode:org; fill-column:79; -*-
#+title: CSV-SQLite3 Using Node
#+subtitle:Convert CSV files into data usable by SQLite3@@texinfo:@*@@
#+subtitle:{{{date}}} {{{version}}}
#+date:2019-07-29 18:46
#+macro: version Version 0.0.21

* Introduction
:PROPERTIES:
:unnumbered: t
:END:
US Bank has the facility to download bank records in CSV form.  This program is
designed to convert those downloaded CSV files into a form usable by SQLite,
and then to use SQLite to process the data.

{{{heading(Header Lines)}}}

The header lines are:

: "Date","Transaction","Name","Memo","Amount"

{{{heading(Data Columns)}}}

{{{subheading(Date)}}}

A sample date is:

: 1/4/2016

This should be transformed into:

: 2016-01-14

{{{subheading(Transaction)}}}

A =Transaction= is one of

- =DEBIT=
- =CREDIT=

This should be transformed into:

: debit | credit

{{{subheading(Name)}}}

A sample =name= entry is:

: DEBIT PURCHASE -VISA USPS PO BOXES 66800-3447779 DC

This should be transformed into:

: visa purchase usps po boxes

{{{subheading(Memo)}}}

A sample =memo= entry is:

: Download from usbank.com. USPS PO BOXES 66800-3447779 DC

The =Download from usbank.com= should be removed in all cases.  The extraneous
numbers should be removed whenever possible.

{{{subheading(Amount)}}}

- -66.0000

This should be transformed into:

: $-66.00

* SQLite Tables
#+cindex:tables
The minimum SQLite tables that should be created are:

- usb :: includes business (6815), trust (6831), personal (6151) data
- cases :: 190301, etc.
- people :: John Doe, Mary Jane, etc.
- businesses :: Law Office of ..., etc.


More can be created as needed.

** SQLite Table Columns
#+cindex:columns
The columns that should be created for the bank tables are:

- =rowid= (implicit creation)
- =date= in the form of =yyyy-mm-dd=
- =trans= containing either =CREDIT | DEBIT=
- =checkno= containing a check number, if present (=check= is a reserved word
  and throws an error)
- =txfr= containing a direction arrow (=<= or =>=) and a bank account (=usb_6151=)
- =payee=
- =category=
- =memo=
- =desc1=
- =desc2=
- =caseno= containing a related case number (=case= is apparently a reserved
  word and throws an error)
- =amount= in the form =\pm##,###.##=


| rowid       | date       | trans  | checkno | txfr       | payee    | category | memo | desc1 | desc2 | caseno  | amount     |
|-------------+------------+--------+---------+------------+----------+----------+------+-------+-------+---------+------------|
| primary key | yyyy-mm-dd | credit | ####    | < usb 6151 | text     | text     | text | text  | text  | integer | \pm##,###.## |
| implicit    | not null   | debit  | null    | > usb 6831 | not null | null     | null | null  | null  | null    | not null   |
| creation    |            |        |         | null       |          |          |      |       |       |         |            |
|-------------+------------+--------+---------+------------+----------+----------+------+-------+-------+---------+------------|

#+name: usb_table_schema
#+begin_src sql
  usb (
         rowid    INTEGER PRIMARY KEY NOT NULL,
         date     VARCHAR NOT NULL,
         trans    VARCHAR NOT NULL,
         checkno  VARCHAR,
         txfr     VARCHAR,
         payee    VARCHAR NOT NULL,
         category VARCHAR,
         note     VARCHAR,
         desc1    VARCHAR,
         desc2    VARCHAR,
         caseno   VARCHAR,
         amount   REAL NOT NULL,
         OrigPayee VARCHAR NOT NULL,
         OrigMemo  VARCHAR NOT NULL )
#+end_src

* CSV-SQLite3 Usage
- ~csv-sqlite3 --csv <usb-acct> <year> [--attach <db>]

  - <usb-acct> is one of: =6815|6831|6151= 

  - <year> is one of =2016|2017|2018= 

  - ~--attach <db>~ is optional; the default is ~workfin.sqlite~

  - Transform downloaded data and save in a new CSV file and a new sqlite3
    database.  The source CSV files are found at:

    ~$WORKFIN/sources/usb/usb_{6815|6831|6151}/{2016|2017|2018}/usb_6815--2016.csv~

  - The transformed CSV files are found at:

    ~$WORKFIN/db/csv/usb_{6815|6831|6151}__{2016|2017|2018}.csv~

  - The new SQLite3 database is found at:

    ~$WORKFIN/db/workfin/sqlite~

- ~csv-sqlite3 --delete [<db>]~ ::

     Delete the SQLite3 database file ~<db>.sqlite~, or ~workfin.sqlite~ if left
     blank.

- ~csv-sqlite3 --log-level <level>~

  Set a log level (default is =warn=)

* Create the Project
This project's dependencies are the following Node.js modules:

- ~command-line-args~ :: https://github.com/75lb/command-line-args#readme
- ~command-line-usage~ :: https://github.com/75lb/command-line-usage
- ~csv~ :: https://csv.js.org/
- ~sqlite3~ :: https://github.com/mapbox/node-sqlite3/wiki
- ~accounting~ :: http://openexchangerates.github.io/accounting.js/

#+name:tangle-CSV-SQLite3-project
#+begin_src emacs-lisp :results output :exports results
(org-babel-tangle-file "CSV-SQLite3.org")
#+end_src

** Install the Dependencies
The following Node dependency modules are installed:

- ~command-line-ars~
- ~command-line-usage~
- ~csv~
- ~sqlite3~
- ~accounting~


#+name:create-CSV-SQLite3-project
#+header: :exports both :results output
#+begin_src sh
yarn --yes init
yarn add command-line-args command-line-usage csv sqlite3 accounting
#+end_src

** Establish some Basic Dependencies
:PROPERTIES:
:header-args: :comments both
:END:
In addition to the foregoing dependencies, this project uses the following
Node.js built-in modules:

- ~fs~ :: File System for working with files

- ~util~ :: Utilities for inspectint objects

#+name:csv-sqlite3-dependencies
#+header: :mkdirp yes
#+begin_src js -n :tangle index.js
const fs   = require('fs');
const util = require('util');

const cl_args  = require('command-line-args');
const cl_usage = require('command-line-usage');
const csv      = require('csv');
const sqlite3  = require('sqlite3').verbose();      // remove 'verbose' in production
const accounting = require('accounting');
#+end_src

** Establish Database Table Name and Columns
#+name:database-table-columns
#+begin_src js +n :tangle index.js
  const DB_TABLES = {
      usb: 'usb',
  };

  const DB_COLS = [
      'date',
      'trans',
      'checkno',
      'txfr',
      'payee',
      'category',
      'note',
      'desc1',
      'desc2',
      'caseno',
      'amount',
      'OrigPayee',
      'OrigMemo',
  ];
#+end_src

* Working with the Command Line
:PROPERTIES:
:header-args: :comments both
:END:
Here is implementation of command-line argument parsing and the generation of a
usage message triggered by requesting the option =--help=.

** Command Line Usage
#+cindex:command-line usage
#+cindex:usage
#+cindex:@command{command-line-usage}
This section generates a usage message activated by the =--help= option.  It
uses the [[option-defs-variable][~options_defs~]] object created in the code below.

#+name:csv-sqlite3-usage
#+begin_src js
const sections = [
    {
        header: 'CSV-SQLite',
        content: 'Processes raw usb csv files into a form usable by SQLite3'
    },
    {
        header: 'Options',
        optionList: option_defs,
    },
    {
        content: `Project directory: {underline ${process.env.WORKNODE}/CSV-SQLite3}`
    }
];
const usage = cl_usage(sections);
console.log(usage);
#+end_src

** Command Line Argument Processing
#+cindex:command-line arguments
#+cindex:arguments
#+cindex:@command{command-line-arguments}
#+cindex:@option{--help}
#+cindex:@option{--create}
#+cindex:@option{--delete}
Options include giving the name of a database to attach to using =--attach
<db>=.  In the absence of this option, a default database will be used.  A
database can be deleted here as well using the option =--delete <db>=, with a
backup being saved in the =WORKBAK= directory wtih the unix time suffixed to
the end.

Initially, the database should be creatable and deleteable.

: csvsqlite --help | -h

: csvsqlite --attach <db> | -a <db-name>

: csvsqlite --delete <db> | -d <db-name>

Also, identify the CSV file to transform via the =--csv= option:

: csvsqlite --csv | -c 6815|6831 2004...2019

<<option-defs-variable>>
#+name:csv-sqlite3-command-line-arg-processing
#+header: :noweb yes
#+begin_src js +n :tangle index.js
  const option_defs = [
      { name: 'help',   alias: 'h', type: Boolean, description: 'Prints this usage message.' },
      { name: 'attach', alias: 'a', type: String,  description: 'Attach to an existing or new database file.' },
      { name: 'delete', alias: 'd', type: String,  description: 'Delete an existing database file and related CSV files.' },
      { name: 'csv',    alias: 'c', type: String,  description: 'Process a CSV file [6815|6831] yyyy', multiple: true  },
      { name: 'export', alias: 'e', type: String,  description: 'Export sqlite3 data into a csv file (default \'workfin.csv\'' },
      { name: 'log-level', alias: 'l', type: Number, description: 'Set a log level 0..10' },
  ];
  const options = cl_args(option_defs);
  console.log(options);

  if (options.help) {
      <<csv-sqlite3-usage>>
      process.exit(0);
  }

  let LOG_LEVEL = process.env.LOG_LEVEL || 1;
  if (options['log-level'] >= 0) {
      if (typeof options['log-level'] === 'number' && options['log-level'] <= 10)
          LOG_LEVEL = options['log-level'];
      else {
          console.error(`Incorrect log-level: ${options['log-level']}; must be between 0 and 10`);
      }
  }
  console.log(`Log-level set at: ${LOG_LEVEL}`);
#+end_src

* Attach To or Delete a Database
:PROPERTIES:
:header-args: :comments both
:END:
SQLite3 can have any number of databases.  Only one is initially attached, but
more can be attached subsequent to the first attachment.  If the database does
not exist, it will be created.  If the user requests that a database file be
deleted, it will be backed up first, then deleted.

The user can attach to a database file (either a specified file or the default
file, defined as ~$WORKFIN/workfin.db~), or delete a specified database file
and the associated CSV file exported from the database data.  All deleted files
are backed up to a backup directory that needs to be defined as a shell
environment variable: =WORKBAK=.  The backed-up files are appended with the
current date in UNIX time (seconds).

#+cindex:@file{db} database file
#+cindex:database file @file{db}
The attached database will be referenced as ~db~.

{{{heading(Verbose Mode)}}}

#+cindex:verbose mode
During development, call the ~verbose()~ method on the ~sqlite3~ object to
enable better stack traces.  In production, remove this call for improved
performance.

#+name:csv-sqlite3-create-database
#+header: :noweb yes
#+begin_src js +n :tangle index.js

  if ( !process.env.WORKDB ) {
      console.error('You must define a shell variable named WORKDB as a base directory for the database file.');
      process.exit(1);
  }

  const WORKDB = process.env.WORKDB; // base directory for db
  const DB_DEFAULT = 'workfin';
  const db_file = options.attach ? options.attach :    // for attaching
                  options.delete ? options.delete :    // for deletion
                  DB_DEFAULT;  	       	         // use the default name
  const db_path = `${WORKDB}/${db_file}.sqlite`;
  const exported_csv = `${WORKDB}/${db_file}.csv`;
  console.log(`db_path: ${db_path}`);
  console.log(`exported_csv: ${exported_csv}`);

  /*---DELETE--*/
  if (options.hasOwnProperty('delete')) {
      if (!process.env.WORKBAK) {
          console.error('You must define a shell variable named WORKBAK as a backup directory before deleting a database file.');
          process.exit(1);
      }
      const WORKBAK = process.env.WORKBAK;

      /* DB */
      const db_path_bak = `${WORKBAK}/${db_file}.sqlite.${Date.now()}`;
      const exported_csv_bak = `${WORKBAK}/$db_file}.csv.${Date.now()}`;
      try {
          fs.renameSync(db_path, db_path_bak);
          console.error(`Renamed ${db_path} to ${db_path_bak}`);
          fs.renameSync(exported_csv, exported_csv_bak);
          console.error(`Renamed ${exported_csv} to ${exported_csv_bak}`);
      } catch (err) {
          if (err.code === 'ENOENT')
              console.log(`file ${db_path} and/or ${exported_csv} did not exist; ignoring.`);
          else {
              throw err;
          }
      }

      /* CSV */
      const db_csv_path = `${WORKDB}/csv`;
      try {
          const files = fs.readdirSync(db_csv_path);
          files.forEach(file => {
              const db_csv_path_file = `${WORKDB}/csv/${file}`;
              const db_csv_path_bak  = `${WORKBAK}/${file}.${Date.now()}`;
              fs.renameSync(db_csv_path_file, db_csv_path_bak);
              console.log(`Renamed ${db_csv_path_file} to ${db_csv_path_bak}`);
          });
          fs.rmdirSync(db_csv_path);
      } catch (err) {
          if (err.code === 'ENOENT') {
              console.log(`${db_csv_path} probably does not exist`);
          } else if (err.code === 'ENOTEMPTY') {
              console.error(`${db_csv_path} does not seem to be empty`);
          } else {
              throw err;
          }
      }

      /* Ledger */
      const ledger_path = `${WORKDB}/ledger`;
      try {
          const files = fs.readdirSync(ledger_path);
          files.forEach(file => {
              const ledger_file = `${ledger_path}/${file}`;
              const ledger_file_bak = `${WORKBAK}/${file}.${Date.now()}`;
              fs.renameSync(ledger_file, ledger_file_bak);
              console.log(`Renamed ${ledger_file} to ${ledger_file_bak}`);
          });
          fs.rmdirSync(ledger_path);
          console.log(`Removed ${ledger_path} from ${WORKDB}`);
      } catch (err) {
          if (err.code === 'ENOENT') {
              console.log(`${ledger_path} probably does not exist`);
          } else if (err.code === 'ENOTEMPTY') {
              console.error(`${ledger_path} does not seem to be empty`);
          } else {
              throw err;
          }
      }

      process.exit(0);
  }

  /*--ATTACH--*/
  // attach in all situations except --delete
  console.log('attaching...');
  const db = new sqlite3.Database(db_path, (err) => {
      if (err)
          console.error(`Error opening database file ${db_path}: ${err.message})`);
      else
          console.log(`Successfully attached to database file ${db_path}`);
  });
  console.log('done');

  db.serialize();

  db.run(`CREATE TABLE IF NOT EXISTS
      <<usb_table_schema>>`);
#+end_src

* Export SQLite DB Data to CSV File
To export the SQLite3 database data to a CSV file, use the
{{{option(--export)}}} option:

: --export [<csv-file>]

The csv filename is optional, with the default being the same as the db file
(i.e., ~workfin~), with the extension ~.csv~, (i.e., ~workfin.csv~) in the
~WORKDB~ directory.  The export is accomplished by executing in a child process
the command line program:

: sqlite3 -header -csv db sql-statements

The process runs the command, and connects the =STDOUT= stream to the CSV file.
Since the entire database is exported, the output will truncate the csv file
upon opening it for writing.  The program will halt after the export.

Upon an export of the SQLite3 data to a CSV file, the program will also send
the exported data through the Ledger ~convert~ command and into the
~workfin.ledger~ data file.  This file is located in the ~ledger/~ directory
below the ~db~ directory.

#+name:csv-sqlite3-export-option
#+begin_src js +n :tangle index.js

  /*--EXPORT--*/
  if (options.hasOwnProperty('export')) {
      const { spawnSync } = require('child_process');
      const export_csv = options['export'] === null ? db_file : options['export'];
      console.log(`exporting to ${export_csv}...`);

      const export_csv_path = `${WORKDB}/csv`;
      if (!fs.existsSync(export_csv_path)) {
          fs.mkdirSync(export_csv_path);
          console.log(`Created ${export_csv_path}`);
      }
      const export_csv_file = `${export_csv_path}/${export_csv}.csv`;

      /*--TRUCATE export_csv UPON OPENING FOR WRITING--*/
      let fd = fs.openSync(export_csv_file,'w');

      let ret = spawnSync(
          'sqlite3',
          [
              '-header',
              '-csv',
              db_path,
              'SELECT rowid,date,trans,checkno,txfr,payee,category,note,caseno,amount FROM usb;'
          ],
          {
              encoding: 'utf-8',
              stdio: [0,fd,1]
          }
      );

      if (ret.error) {
          console.log(`status: ${ret.status}\tsignal: ${ret.signal}`)
          console.log(`error: ${ret.error}`);
      }

      console.log('done exporting');
      fs.closeSync(fd);

      const ledger_path = `${process.env.WORKDB}/ledger`;
      const ledger_file = `${ledger_path}/${export_csv}.ledger`;
      const zero_file   = `${ledger_path}/zero.ledger`;
      if (!fs.existsSync(ledger_path)) {
          fs.mkdirSync(ledger_path);
      }

      if (!fs.existsSync(zero_file)) {
          fs.writeFileSync(zero_file,''); // needed by the convert command??
      }

      fd = fs.openSync(`${ledger_file}`,'as');
      ret = spawnSync(
          'ledger',
          [
              'convert',
              `${export_csv_file}`,
              '--input-date-format=%Y-%m-%d',
              '--account=Assets:USB',
              '--rich-data',
              `--file=${zero_file}`,
              `--now=${(new Date()).toISOString().split('T')[0]}`,
          ],
          {
              encoding: 'utf-8',
              stdio: [0,fd,2],
              cwd: `${WORKDB}`,
          }
      );


      if (ret.error) {
          console.log(`status: ${ret.status}\tsignal: ${ret.signal}`)
          console.log(`error: ${ret.error}`);
      }

      process.exit(0);
  }

  /*--DON'T CONTINUE UNLESS --csv OPTION USED--*/
  if (!options.hasOwnProperty('csv'))
      process.exit(0);
#+end_src

* Process CSV Files
:PROPERTIES:
:header-args: :comments both
:END:
The Node.js module [[https://csv.js.org/][~csv~]] contains the

- [[https://csv.js.org/parse/][csv-parser]],

- [[https://csv.js.org/transform/][csv-stream-transformer]],

- [[https://csv.js.org/stringify/][csv-stringifier]],


{{{noindent}}} all of which will be used in this project.  The pattern is to
open a CSV file, parse a CSV string into records and pipe those records through
the transformer to be massaged into shape.  From there the new data is saved in
another CSV file and also an SQLite3 database using

-  [[https://www.npmjs.com/package/sqlite3][~sqlite3~]] (see its [[https://github.com/mapbox/node-sqlite3/wiki/API][API]] also)


The processing of a CSV file, therefore, involves the following steps and
Node.js modules:

1. Find the correct CSV file (using ~FileSystem~) and open it as a [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_readable_streams][Readable
   Stream]];

  - [[*Set Up StreamReader][Set Up StreamReader]]

2. Open a new CSV file to hold the new transformed data as a [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_writable_streams][Writable Stream]]

   - [[*Set Up CSV-Stringify][Set Up CSV-Stringify]]

3. Open an SQLite3 database to hold the new transformed data

   - [[*Attach To or Delete a Database][Attach To or Delete a Database]]

4. Read the CSV records from the file as a string (using ~StreamReader~)

   - [[*Set Up StreamReader][Set Up StreamReader]]

5. Parse the string into JS records (using ~CSV-Parse~)

   - [[*Set Up CSV-Parse][Set Up CSV-Parse]]

6. Transform the JS records into usable data (using ~CSV-Transform~)

   - [[*Set Up the Stream Transform][Set Up the Stream Transform]]

   - [[*Set Up the Transform Function][Set Up the Transform Function]]

7. Save the new data in the new CSV file (using ~StreamWriter~)

   - [[*Set Up CSV-Stringify][Set Up CSV-Stringify]]

8. Save the new data in an SQLite3 database (using ~SQLite3~)

   - [[*Set Up the Stream Transform][Set Up the Stream Transform]]

** Set Up CSV-Stringify
This section receives the transformed records from the Transform function and
writes them to new csv files.  The new csv files will be located close to the
database files, so there should be an environment variable named =WORKDB=,
pointing to, for example, ~$WORK/workfin/db~.  A file will be called, for
example, ~usb_6815__2016.csv~.  Notice that this file name uses two
underscores, whereas the source files use two dashes; in all other respects,
they are the same.

#+name:csv-stringify-function
#+begin_src js +n :tangle index.js
  const stringifier = csv.stringify({
      header: true,
      columns: [
          'date',
          'trans',
          'checkno',
          'txfr',
          'payee',
          'category',
          'note',
          'desc1',
          'desc2',
          'caseno',
          'amount',
          'OrigPayee',
          'OrigMemo',
          ],
  });

  const acct = options.csv[0],
        year = options.csv[1];

  const csv_file = `usb_${acct}__${year}.csv`;
  const csv_path = `${process.env.WORKDB}/csv`;
  const csv_path_file = `${csv_path}/${csv_file}`;
  if (! fs.existsSync(csv_path)) {
      try {
          fs.mkdirSync(csv_path);
          console.log(`CSV FILE PATH: ${csv_path} has been created`);
      } catch (err) {
          console.error(err.message);
          process.exit(1);
      }
  }
  console.log(`CSV FILE PATH: ${csv_path} exists`);

  let csv_stringifier;
  try {
      csv_stringifier = fs.createWriteStream(csv_path_file);
      console.log(`WRITE STREAM: ${csv_path_file} has been successfully opened.`);
  } catch (err) {
      console.error(err.message);
      process.exit(1);
  }

  stringifier.on('readable', function() {
      console.log('stringifier is now readable');
      let row;
      while (row = this.read()) {
          console.log(`stringifer row: ${row}`);
          csv_stringifier.write(row);
      }
  });

  stringifier.on('error', function(err) {
      console.error(err.message);
  });

  stringifier.on('finish', function() {
      console.log('stringifier is done writing to csv_stringifer');
      csv_stringifier.end('stringifer called csv_stringifier\'s "end" method');
  });

  stringifier.on('close', function() {
      console.log('stringifier is now closed');
  });

  csv_stringifier.on('close', function() {
      console.log('csv_stringifier is now closed');
  });
#+end_src

** Set Up Stream-Transform and Transform Function
:PROPERTIES:
:header-args: :comments both
:END:
This code implements the stream transformer functionality, which is at the
heart of this project.

The Transformer is a [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_class_stream_transform][Node.js Transform Stream]].  This means it is capable of
both reading and writing data.  In this project, it [[csv-transformer-write-method%0A][writes data]] in the CSV
Parser, and then reads it here via its ~transformer.read()~ method.  This
~transformer~ object has a ~transform()~ method that takes a function callback,
whose purpose is to to /transform/ records that are read.  This is the heart of
this project.  The ~transform()~ function is implemented in the following
section, and returns completely transformed CSV bank records at its end.  These
transformed records are then written to both a new CSV file, and the SQLite3
database.

#+attr_texinfo: :options CSV transform ( transform_callback )
#+begin_defmethod
The CSV ~transform~ method reads a record and sends that record to a
=TRANSFORM_CALLBACK= that is used to /transform/ the data.
#+end_defmethod

After it transforms the data, the transformer receives the new data via a
=readable= event, where it can process the data.

#+cindex:@code{INSERT} into @file{db}
#+cindex:@command{db.run}
The transformed data will also be saved into the SQLite3 database via an
=INSERT= statement executed by the ~db.run()~ method.

*** Set Up the Transform Function
The Transform Function receives a record and massages it into shape.  The
following regular expressions were created based upon inspection of the raw
data as it came from the bank for years 2016, 2017, and 2018.  It does a decent
job of creating readable payees and memos, as well as txfrs (transfers), but it
has not been set up to do anything for check payees, categories or related
records, for example.

#+name:stream-transform-function
#+begin_src js +n :tangle index.js
  const transform_function = function (record) {
      const DEBIT   = 'debit';
      const CREDIT  = 'credit';
      const CHECK   = 'check';
      const CASH    = 'cash';
      const DEPOSIT = 'deposit';
      const UNKNOWN = 'unknown';
      const TRANS    = 'transfer';
      const USBANK  = 'usbank';
      let   trfrom  = '';

      // Add new columns: checkno, txfr, acct, _case, desc1, desc2, category
      record.checkno = null; // check no.
      record.txfr    = null; // direction and acct #
      record.acct    = null; // related account foreign key
      record.caseno  = null; // related case foreign key
      record.desc1   = null; // noun
      record.desc2   = null; // adjective
      record.category= null; // categorization of the transaction

      // Format date as yyyy-mm-dd; delete original Date
      record.date = new Date(record['Date']).toISOString().split('T')[0];
      delete record['Date'];

      // Change Transaction to trans; delete original Transaction
      record.trans = record['Transaction'].toLowerCase();
      delete record['Transaction'];

      // Change Amount to amount as Currency type; delete original Amount
      record.amount = accounting.formatMoney(record['Amount']);
      delete record['Amount'];

      // Change Name to payee; keep original Name as OrigName; delete Name
      record.payee = record['Name'].toLowerCase().trimRight();
      record.OrigPayee = record['Name'];
      delete record['Name'];

      // Clean up Memo by removing Download message; return as note; keep Memo as OrigMemo
      let re = new RegExp('Download from usbank.com.\\s*');
      record.note = record['Memo'].replace(re,'').toLowerCase();
      record.OrigMemo = record['Memo'];
      delete record['Memo'];

      // Add check no. to checkno column
      if (record.payee === CHECK) {
          const checkno = record.trans.replace(/^0*/,'');
          record.checkno  = checkno;
          record.trans   = DEBIT;
          record.payee  = UNKNOWN;
          record.note  += `Purchase by check no. ${checkno}`;
          record.desc1  = 'purchase';
          record.desc2  = 'check';
      }

      if (record.payee.match(/(returned) (item)/)) {
          record.desc1 = RegExp.$2;
          record.desc2 = RegExp.$1;
          record.payee = USBANK;
          record.note = `${record.desc2} ${record.desc1}`;
      }

      if (record.payee.match(/(internet|mobile) (banking) transfer (deposit|withdrawal) (\d{4})\s*$/)) {
          record.desc1 = RegExp.$3;
          record.desc2 = RegExp.$1;
          record.txfr = `${(RegExp.$3 === 'deposit') ? '<' : '>'} usb_${RegExp.$4}`;
          tofrom = (record.trans === 'debit') ? 'to' : 'from';
          record.payee = (record.trans === 'debit') ? `usb_${RegExp.$4}` : `usb_${options.csv[0]}`;
          record.note = `${record.desc2} ${record.desc1}: ${TRANS} ${tofrom} ${record.note}`;
      }

      if (record.payee.match(/debit (purchase)\s*-?\s*(visa)? /)) {
          record.desc1 = RegExp.$1;
          record.desc2 = RegExp.$2;
          record.payee = record.payee.replace(RegExp.lastMatch,'');
          record.note = `${record.desc2} ${record.desc1} ${record.note}`.trimLeft();;
      }

      // web authorized payment
      // atm|electronic|mobile check|rdc deposit|withdrawal <name>
      if (record.payee.match(/(web authorized) (pmt) |(atm|electronic|mobile)?\s*(check|rdc)?\s*(deposit|withdrawal)\s*(.*)?/)) {
          tofrom = '';
          record.desc1 = RegExp.$2 ? RegExp.$2 : RegExp.$4 ? RegExp.$4 : RegExp.$5 ? RegExp.$5 : 'undefined';
          record.desc2 = RegExp.$1 ? RegExp.$1 : RegExp.$3 ? RegExp.$3 : 'undefined';
          if (RegExp.$3 === 'atm' || RegExp.$3 === 'electronic' || RegExp.$3 === 'mobile' || RegExp.$5 === DEPOSIT) {
              record.payee = (RegExp.$5 === 'deposit') ? `usb_${options.csv[0]}` : CASH;
          } else {
              record.payee = record.payee.replace(RegExp.lastMatch,'');
          }
          if (record.note.match(/paypal/) && record.trans === CREDIT) {
              record.txfr = `< ${RegExp.lastMatch}`;
              tofrom = ' from';
          }
          record.note = `${record.desc2} ${record.desc1}${tofrom} ${record.note}`.trimRight();
      }

      if (record.payee.match(/(zelle instant) (pmt) (from (\w+\s\w+))\s(.*)$/)) {
          record.desc1 = RegExp.$2;
          record.desc2 = RegExp.$1;
          record.note = `${record.desc2} ${record.desc1} ${RegExp.$3}`;
          record.payee = `usb_${options.csv[0]}`;
      }

      if (record.payee.match(/(overdraft|international) (paid|processing) (fee)/)) {
          record.desc1 = RegExp.$3;
          record.desc2 = `${RegExp.$1} ${RegExp.$2}`;
          record.payee = USBANK;
          record.note  = `${record.desc2} ${record.desc1} to ${record.payee}`;
      }

      record.payee = record.payee.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.note  = record.note.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.payee = record.payee.replace(/\s\d{3}\w+\s{2,}or$/,''); // Nike Company 019Beaverton   OR
      record.note  = record.note.replace(/\s\d{3}\w+\s{2,}or$/,'');
      record.payee = record.payee.replace(/\s*[-\d]{5,}\s*\w{2}$/,''); // '650-4724100 CA' & '        855-576-4493WA' & '  800-3333330 MA'
      record.note  = record.note.replace(/\s*[-\d]{5,}\s*\w{2}$/,'');
      record.payee = record.payee.replace(/(\s\w*https)?www.*$/,''); // WWW.ATT.COM TX; UDEMY ONLINE COUHTTPSWWW.UDECA
      record.note  = record.note.replace(/(\s\w*https)?www.*$/,'');
      record.payee = record.payee.replace(/\s*\w+\.com\s+\w{2}$/, '');
      record.note  = record.note.replace( /\s*\w+\.com\s+\w{2}$/, '');
      record.payee = record.payee.replace(/aws.amazon.cWA/i,''); // serviaws.amazon.cWA
      record.note  = record.note.replace(/aws.amazon.cWA/i,'');
      if (record.payee.match(/(bostype \/ wes bo)(hamilton\s+on)/)) { // WES BOHAMILTON    ON
          record.payee = 'Wes Bos';
          record.note  = record.note.replace(RegExp.$1,'Wes Bos');
          record.note  = record.note.replace(RegExp.$2, '');
      }
      record.payee = record.payee.replace(/\s{2,}/g,' ');
      record.note  = record.note.replace(/\s{2,}/g,' ');

      /*
        'DEBIT PURCHASE -VISA SQ *PHIL        877-417-4551WA'

        You paid Phil $159 for Atreus keyboard kit and shipping

        It is for a credit card processor that goes by the brand name
        Square Up. Merchants can run credit card transactions through
        their iPhone or iPads using the Square Up services. Mine was for
        a taxi ride. https://800notes.com/Phone.aspx/1-877-417-4551
      ,*/

      record.payee = record.payee.replace(/sq/, 'square');
      record.note  = record.note.replace(/sq/, 'square');

      return record;
  }
#+end_src

#+RESULTS: stream-transform-function
: undefined

*** Set Up the Stream Transform
#+name:stream-transformer
#+begin_src js +n :tangle index.js
  const transformer = csv.transform(transform_function)

  /* TRANSFORMER reads records through its TRANSFORM_FUNCTION */
  /* -------------------------------------------------------- */
  transformer.on('readable', function() {
      let record;
      while ((record = transformer.read())) {
          console.log(`Transformer record:\n${util.inspect(record)}`);

          /* STRINGIFIER WRITE Records */
          /* ------------------------- */
          stringifier.write(record);



          /* DB RUN---INSERT RECORDS */
          /* ----------------------- */
          const tab_name  = DB_TABLES['usb'];
          const col_names = DB_COLS.join(',');
          const col_phs   = DB_COLS.map(c => '?').join(',');
          const col_values= DB_COLS.map(c => record[c]);

          let sql = `INSERT INTO ${ tab_name }( ${ col_names } )
                     VALUES ( ${ col_phs } )`

          console.log(`sql: ${ sql }`);
          console.log(`col_values: ${ col_values }`);

          db.run(sql, col_values, (err) => {
             if (err) {
                 console.error(err.message);
                 console.error(`ERROR sql: ${ sql }`);
                 console.error(`ERROR values: ${ col_values }`);
                 process.exit(1);
             }
         });
     }
  });

  transformer.on('error', function(err) {
      console.error(err.message);
  });

  transformer.on('finish', function() {
      console.log('Transformer finished writing records.');
  });

  transformer.on('end', function() {
      console.log('Transformer end reached.');
      stringifier.end();
  });
#+end_src

** Set Up CSV-Parse
:PROPERTIES:
:header-args: :comments both
:END:
#+cindex:@code{write} method, transformer
This section implements the csv parser.  By default, it does little other than
read a large string of data and parse it into an array of records.  By giving
it the option =columns = true=, however, the parser will use the first line as
a list of column headings, and produce an array of objects where the keys are
column names, and the values are column entries.  Each record is written to the
stream transformer using its =WRITE= method.

<<csv-transformer-write-method>>
#+name:csv-sqlite3-csv-parse
#+header: :noweb yes
#+header: :comments link
#+begin_src js +n :tangle index.js
const parser = csv.parse({columns: true});
const records = [];

parser.on('readable', function() {
    console.log('Parser beginning to read records.');
    let record;

    /* PARSE A RECORD AND WRITE TO THE TRANSFORMER */
    while ((record = parser.read())) {
        console.log(`parser record:\n${util.inspect(record)}`);
        transformer.write(record);
    }

});

parser.on('error', function(err) {
    console.error(err.message);
});

parser.on('end', function() {
    console.log('Parser finished reading records.');
});

parser.on('finish', function () {
    console.log('Parser finished writing records.');
    console.log('Parser calling transformer end');
    transformer.end();
});
#+end_src

** Set Up StreamReader
This section implements the Stream Reader that reads the CSV file in as a large
string of data and sends it to the csv parser via the parser's ~write~ method.

CSV financial files are found in the directories =$WORKUSB_[6815|6831]/yyyy=,
where =yyyy= can be 2004--2019, and on.  Given =[6815|6831]= and a year
=[2004|2005...2019]=, the file path will be
=$WORKUSB_6815/YYYY/usb_6815--yyyy.csv=.  This code makes sure the file exists
and the user has proper permissions to read it before proceeding.

#+name:csv-sqlite3-process-csv-files
#+header: :noweb yes
#+begin_src js +n :tangle index.js
  if (options.csv) {
      const acct = options.csv[0],
            year = options.csv[1];

      if (!process.env.WORKUSB) {
          console.error('You must assign a path to the shell variable WORKUSB');
          process.exit(1);
      }

      const acct_year_path = `${process.env.WORKUSB}/usb_${acct}/${year}`;
      const acct_year_csv_file = `usb_${acct}--${year}.csv`;
      const acct_year_csv_file_path = `${acct_year_path}/${acct_year_csv_file}`;
      if (!fs.existsSync(acct_year_csv_file_path) || !(fs.accessSync(acct_year_csv_file_path, fs.constants.R_OK) === undefined)) {
          console.error(`Cannot find or access the CSV file at '${acct_year_csv_file_path}'.`);
          process.exit(1);
      }
      console.log(`Successfully found the CSV file: '${acct_year_csv_file_path}'`);

      /* CREATE THE STREAM HERE */
      const csv_file_stream = fs.createReadStream(acct_year_csv_file_path, {encoding: 'utf8'});

      /* Set up streaming events 'READABLE', 'ERROR', and 'END' */
      csv_file_stream.on('readable', function () {
          let record;

          /* READ THE RECORDS */
          while ((record = this.read())) {
              console.log(`readable record: ${record}`);

              /* WRITE A RECORD TO THE PARSER */
              parser.write(record);

          }
          parser.end();

      });

      csv_file_stream.on('error', function(err) {
          console.error(err.message);
      });

      csv_file_stream.on('end', function () {
          console.log('Reader finished reading data.');
      });
  }
#+end_src

* Create Tables

* Node-SQLite3 Module
:PROPERTIES:
:appendix: true
:END:
Asynchronous, non-blocking SQLite3 bindings for Node.js.

- [[https://github.com/mapbox/node-sqlite3][Github]]

- [[https://github.com/mapbox/node-sqlite3/wiki/API][Wiki API]]

** Node-SQLite3 Module Usage
#+name:node-sqlite3-module-sample-usage
#+begin_src js -n
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(':memory:');

db.serialize(function() {
  db.run("CREATE TABLE lorem (info TEXT)");

  var stmt = db.prepare("INSERT INTO lorem VALUES (?)");
  for (var i = 0; i < 10; i++) {
      stmt.run("Ipsum " + i);
  }
  stmt.finalize();

  db.each("SELECT rowid AS id, info FROM lorem", function(err, row) {
      console.log(row.id + ": " + row.info);
  });
});

db.close();
#+end_src

** Features
- Straightforward query and parameter binding interface
- Full Buffer/Blob support
- Extensive debugging support
  #+cindex:serialization
- Query serialization API
- Extension support
- Big test suite
- Written in modern C++ and tested for memory leaks
- Bundles Sqlite3 3.26.0 as a fallback if the installing system doesn't include
  SQLite

** Node-SQLite3 API
#+cindex:serialization, function call
~node-sqlite3~ has built-in /function call serialization/ and automatically waits
before executing a blocking action until no other action is pending.  This
means that it's safe to start calling functions on the database object even if
it is not yet fully opened.  The ~Database#close()~ function will wait until
all pending queries are completed before closing the database.

** Node-SQLite3 Control Flow---Two Exeuction Modes
#+cindex:execution flow
#+cindex:parallel execution
#+cindex:exclusive mode
~node-sqlite3~ provides two functions to help control the execution flow of
statements.  The default mode is to execute statements in /parallel/.  However,
the ~Database#close~ method will always run in /exclusive mode/, meaning it
waits until all previous queries have completed and ~node-sqlite3~ will not run
any other queries while a ~close~ is pending.

*** Serialize Execution Mode
#+cindex:execution mode, serialize
#+cindex:serialize execution mode
#+cindex:serialized mode

#+attr_texinfo: :options Database serialize ( [callback] )
#+begin_defmethod
Puts the /execution mode/ into /serialized mode/.  This means that at most one
statement object can execute a query at a time.  Other statements wait in a
queue until the previous statements are executed.

If a callback is provided, it will be called immediately.  All database queries
scheduled in that callback will be serialized.  After the function returns, the
database is set back to its original mode again.
#+end_defmethod

Calling ~Database#serialize()~ within nested functions is safe:

#+name:node-sqlite-3-serialized-mode-example
#+begin_src js -n
  // Queries scheduled here will run in parallel.

  db.serialize(function() {

      // Queries scheduled here will be serialized.
      db.serialize(function() {
          // Queries scheduled here will still be serialized.
      });
      // Queries scheduled here will still be serialized.
  });

  // Queries scheduled here will run in parallel again.

#+end_src

Note that queries scheduled not directly in the callback function are not
necessarily serialized:

#+begin_src js -n
  db.serialize(function() {

      // These two queries will run sequentially.
      db.run("CREATE TABLE foo (num)");
      db.run("INSERT INTO foo VALUES (?)", 1, function() {

          // These queries will run in parallel and the second query will probably
          // fail because the table might not exist yet.
          db.run("CREATE TABLE bar (num)");
          db.run("INSERT INTO bar VALUES (?)", 1);
      });
  });
#+end_src

#+cindex:sticky execution mode
#+cindex:execution mode, sticky
If you call it without a function parameter, the execution mode setting is
sticky and won't change until the next call to ~Database#parallelize~.

*** Parallelize Execution Mode
#+cindex:parallized exeuction mode
#+cindex:execution mode, parallelized

#+attr_texinfo: :options Database parallelize ( [callback] )
#+begin_defmethod
Puts the execution mode into parallelized.  This means that queries scheduled
will be run in parallel.

If a callback is provided, it will be called immediately.  All database queries
scheduled in that callback will run parallelized.  After the function returns,
the database is set back to its original mode again.
#+end_defmethod

Calling ~Database#parallelize()~ within nested functions is safe:

#+begin_src js -n
  db.serialize(function() {

      // Queries scheduled here will be serialized.
      db.parallelize(function() {

          // Queries scheduled here will run in parallel.
      });

      // Queries scheduled here will be serialized again.
  });
#+end_src

If you call it without a function parameter, the execution mode setting is
sticky and won't change until the next call to ~Database#serialize~.

* Index
:PROPERTIES:
:unnumbered: t
:index:    cp
:END:

* Function Index
:PROPERTIES:
:index:    fn
:unnumbered: true
:END:

* Macro Definitions                                                :noexport:
#+macro: heading @@texinfo:@heading @@$1
#+macro: subheading @@texinfo:@subheading @@$1
#+macro: noindent @@texinfo:@noindent @@
#+macro: option @@texinfo:@option{@@$1@@texinfo:}@@

* Export Settings                                                  :noexport:
#+texinfo_filename:csv-sqlite3.info
#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:CSV
#+texinfo_dir_title:ConvertCSV (convertcsv)
#+texinfo_dir_desc:Convert USB CSV files to SQLite
#+texinfo_printed_title:ConvertCSV Using Node.js CSV-Parser

* Local Variables                                                  :noexport:
# Local Variables:
# time-stamp-pattern:"8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
