# -*- mode:org; fill-column:79; -*-
#+title: CSV-SQLite3 Using Node
#+subtitle:Convert CSV files into data usable by SQLite3@@texinfo:@*@@
#+subtitle:{{{date}}} {{{version}}}
#+date:2019-07-14 17:44
#+macro: version Version 0.0.8

* Introduction
:PROPERTIES:
:unnumbered: t
:END:
US Bank has the facility to download bank records in CSV form.  This program is
designed to convert those downloaded CSV files into a form usable by SQLite,
and then to use SQLite to process the data.

{{{heading(Header Lines)}}}

The header lines are:

: "Date","Transaction","Name","Memo","Amount"

{{{heading(Data Columns)}}}

{{{subheading(Date)}}}

A sample date is:

: 1/4/2016

{{{subheading(Transaction)}}}

A =Transaction= is one of

- =DEBIT=
- =CREDIT=

{{{subheading(Name)}}}

A sample =name= entry is:

: DEBIT PURCHASE -VISA USPS PO BOXES 66800-3447779 DC

{{{subheading(Memo)}}}

A sample =memo= entry is:

: Download from usbank.com. USPS PO BOXES 66800-3447779 DC

{{{subheading(Amount)}}}

- -66.0000

* SQLite Tables
#+cindex:tables
The minimum SQLite tables that should be created are:

- business
- trust
- personal


More can be created as needed.

** SQLite Table Columns
#+cindex:columns
The columns that should be created for each of the tables are:

- =rowid= (implicit creation)
- =date= in the form of =yyyy-mm-dd=
- =type= containing either =CREDIT | DEBIT=
- =check= containing a check number, if present
- =payee=
- =category=
- =memo=
- =workcase= containing a related case number (=case= is apparently a reserved
  word and throws an error)
- =amount= in the form =\pm##,###.##=


| rowid       | date       | type   | check | payee    | category | memo | workcase | amount     |   |
|-------------+------------+--------+-------+----------+----------+------+----------+------------+---|
| primary key | yyyy-mm-dd | credit | ####  | text     | text     | text | integer  | \pm##,###.## |   |
| implicit    | not null   | debit  | null  | not null | null     | null | null     | not null   |   |
| creation    |            |        |       |          |          |      |          |            |   |
|-------------+------------+--------+-------+----------+----------+------+----------+------------+---|

* Create the Project
This project's dependencies are the following Node.js modules:

- ~command-line-args~ :: https://github.com/75lb/command-line-args#readme
- ~command-line-usage~ :: https://github.com/75lb/command-line-usage
- ~csv~ :: https://csv.js.org/
- ~sqlite3~ :: https://github.com/mapbox/node-sqlite3/wiki
- ~accounting~ :: http://openexchangerates.github.io/accounting.js/

#+name:tangle-CSV-SQLite3-project
#+begin_src emacs-lisp :results output :exports results
(org-babel-tangle-file "CSV-SQLite3.org")
#+end_src

** Install the Dependencies
#+name:create-CSV-SQLite3-project
#+header: :exports both :results output
#+begin_src sh
yarn --yes init
yarn add command-line-args command-line-usage csv sqlite3 accounting
#+end_src

#+RESULTS: create-CSV-SQLite3-project
#+begin_example
yarn init v1.16.0
success Saved package.json
Done in 0.10s.
yarn add v1.16.0
info No lockfile found.
[1/4] Resolving packages...
[2/4] Fetching packages...
[3/4] Linking dependencies...
[4/4] Building fresh packages...
success Saved lockfile.
success Saved 124 new dependencies.
info Direct dependencies
├─ command-line-args@5.1.1
├─ command-line-usage@6.0.2
├─ csv@5.1.1
└─ sqlite3@4.0.9
info All dependencies
├─ abbrev@1.1.1
├─ ajv@6.10.1
├─ ansi-regex@2.1.1
├─ ansi-styles@3.2.1
├─ aproba@1.2.0
├─ are-we-there-yet@1.1.5
├─ asn1@0.2.4
├─ asynckit@0.4.0
├─ aws-sign2@0.7.0
├─ aws4@1.8.0
├─ balanced-match@1.0.0
├─ bcrypt-pbkdf@1.0.2
├─ brace-expansion@1.1.11
├─ caseless@0.12.0
├─ chalk@2.4.2
├─ chownr@1.1.2
├─ code-point-at@1.1.0
├─ color-convert@1.9.3
├─ color-name@1.1.3
├─ combined-stream@1.0.8
├─ command-line-args@5.1.1
├─ command-line-usage@6.0.2
├─ concat-map@0.0.1
├─ console-control-strings@1.1.0
├─ core-util-is@1.0.2
├─ csv-generate@3.2.3
├─ csv-parse@4.4.3
├─ csv-stringify@5.3.0
├─ csv@5.1.1
├─ dashdash@1.14.1
├─ debug@3.2.6
├─ deep-extend@0.6.0
├─ delayed-stream@1.0.0
├─ delegates@1.0.0
├─ detect-libc@1.0.3
├─ ecc-jsbn@0.1.2
├─ escape-string-regexp@1.0.5
├─ extend@3.0.2
├─ extsprintf@1.3.0
├─ fast-deep-equal@2.0.1
├─ fast-json-stable-stringify@2.0.0
├─ find-replace@3.0.0
├─ forever-agent@0.6.1
├─ form-data@2.3.3
├─ fs-minipass@1.2.6
├─ fs.realpath@1.0.0
├─ gauge@2.7.4
├─ getpass@0.1.7
├─ glob@7.1.4
├─ har-schema@2.0.0
├─ har-validator@5.1.3
├─ has-flag@3.0.0
├─ has-unicode@2.0.1
├─ http-signature@1.2.0
├─ iconv-lite@0.4.24
├─ ignore-walk@3.0.1
├─ inflight@1.0.6
├─ inherits@2.0.4
├─ ini@1.3.5
├─ is-fullwidth-code-point@1.0.0
├─ is-typedarray@1.0.0
├─ isarray@1.0.0
├─ isstream@0.1.2
├─ json-schema-traverse@0.4.1
├─ json-schema@0.2.3
├─ json-stringify-safe@5.0.1
├─ jsprim@1.4.1
├─ lodash.camelcase@4.3.0
├─ lodash.get@4.4.2
├─ mime-db@1.40.0
├─ mime-types@2.1.24
├─ minimist@0.0.8
├─ minizlib@1.2.1
├─ mkdirp@0.5.1
├─ ms@2.1.2
├─ nan@2.14.0
├─ needle@2.4.0
├─ node-pre-gyp@0.11.0
├─ nopt@4.0.1
├─ npm-bundled@1.0.6
├─ npm-packlist@1.4.4
├─ npmlog@4.1.2
├─ number-is-nan@1.0.1
├─ oauth-sign@0.9.0
├─ object-assign@4.1.1
├─ os-homedir@1.0.2
├─ os-tmpdir@1.0.2
├─ osenv@0.1.5
├─ path-is-absolute@1.0.1
├─ performance-now@2.1.0
├─ process-nextick-args@2.0.1
├─ psl@1.2.0
├─ punycode@1.4.1
├─ qs@6.5.2
├─ rc@1.2.8
├─ readable-stream@2.3.6
├─ reduce-flatten@2.0.0
├─ request@2.88.0
├─ rimraf@2.6.3
├─ safer-buffer@2.1.2
├─ sax@1.2.4
├─ semver@5.7.0
├─ set-blocking@2.0.0
├─ signal-exit@3.0.2
├─ sqlite3@4.0.9
├─ sshpk@1.16.1
├─ stream-transform@1.0.8
├─ string_decoder@1.1.1
├─ string-width@1.0.2
├─ strip-ansi@3.0.1
├─ strip-json-comments@2.0.1
├─ supports-color@5.5.0
├─ table-layout@1.0.0
├─ tar@4.4.10
├─ tough-cookie@2.4.3
├─ tunnel-agent@0.6.0
├─ tweetnacl@0.14.5
├─ uri-js@4.2.2
├─ util-deprecate@1.0.2
├─ uuid@3.3.2
├─ verror@1.10.0
├─ wide-align@1.1.3
├─ wordwrapjs@4.0.0
└─ yallist@3.0.3
Done in 10.75s.
#+end_example

** Establish some Basic Dependencies
:PROPERTIES:
:header-args: :comments both
:END:
In addition to the foregoing dependencies, this project uses the following
Node.js built-in modules:

- ~fs~ :: File System for working with files

- ~util~ :: Utilities for inspectint objects

#+name:csv-sqlite3-dependencies
#+header: :mkdirp yes
#+begin_src js -n :tangle index.js
const fs = require('fs');
const util = require('util');
#+end_src

* Working with the Command Line
:PROPERTIES:
:header-args: :comments both
:END:
Here is implementation of command-line argument parsing and the generation of a
usage message triggered by requesting the option =--help=.

** Command Line Usage
#+cindex:command-line usage
#+cindex:usage
#+cindex:@command{command-line-usage}
This section generates a usage message activated by the =--help= option.  It
uses the [[option-defs-variable][~options_defs~]] object created in the code below.

#+name:csv-sqlite3-usage
#+begin_src js
const cl_usage = require('command-line-usage');
const sections = [
    {
        header: 'CSV-SQLite',
        content: 'Processes raw usb csv files into a form usable by SQLite3'
    },
    {
        header: 'Options',
        optionList: option_defs,
    },
    {
        content: `Project directory: {underline ${process.env.WORKNODE}/CSV-SQLite3}`
    }
];
const usage = cl_usage(sections);
console.log(usage);
#+end_src

** Command Line Argument Processing
#+cindex:command-line arguments
#+cindex:arguments
#+cindex:@command{command-line-arguments}
#+cindex:@option{--help}
#+cindex:@option{--create}
#+cindex:@option{--delete}
Options include giving the name of a database to attach to using =--attach
<db>=.  In the absence of this option, a default database will be used.  A
database can be deleted here as well using the option =--delete <db>=, with a
backup being saved in the =WORKBAK= directory wtih the unix time suffixed to
the end.

Initially, the database should be creatable and deleteable.

: csvsqlite --help | -h

: csvsqlite --attach <db> | -a <db-name>

: csvsqlite --delete <db> | -d <db-name>

Also, identify the CSV file to transform via the =--csv= option:

: csvsqlite --csv | -c 6815|6831 2004...2019

<<option-defs-variable>>
#+name:csv-sqlite3-command-line-arg-processing
#+header: :noweb yes
#+begin_src js +n :tangle index.js
const cli_args = require('command-line-args');
const option_defs = [
    { name: 'help',   alias: 'h', type: Boolean, description: 'Prints this usage message.' },
    { name: 'attach', alias: 'a', type: String,  description: 'Attach to an existing or new database file.' },
    { name: 'delete', alias: 'd', type: String,  description: 'Delete an existing database file.' },
    { name: 'csv',    alias: 'c', type: String,  description: 'Process a CSV file [6815|6831] yyyy', multiple: true  },
];
const options = cli_args(option_defs);
console.log(options);

if (options.help) {
    <<csv-sqlite3-usage>>
    process.exit(0);
}
#+end_src

* Attach To or Delete a Database
:PROPERTIES:
:header-args: :comments both
:END:
SQLite3 can have any number of databases.  Only one is initially attached, but
more can be attached subsequent to the first attachment.  If the database does
not exist, it will be created.  If the user requests that a database file be
deleted, it will be backed up first, then deleted.

The user can attach to a database file (either a specified file or the default
file, defined as ~$WORKFIN/workfin.db~), or delete a specified database file.
A deleted file is backed up to a backup directory that needs to be defined as a
shell environment variable: =WORKBAK=.

{{{heading(Verbose Mode)}}}

#+cindex:verbose mode
During development, call the ~verbose()~ method on the ~sqlite3~ object to
enable better stack traces.  In production, remove this call for improved
performance.

#+name:csv-sqlite3-create-database
#+begin_src js +n :tangle index.js

  if ( !process.env.WORKDB ) {
      console.error('You must define a shell variable named WORKFDB as a base directory for the database file.');
      process.exit(1);
  }

  const WORKDB = process.env.WORKDB; // base directory for db
  const DB_DEFAULT = 'workfin.sqlite';
  const db_file = options.attach ? options.attach :    // for attaching
                  options.delete ? options.delete :    // for deletion
                  DB_DEFAULT;  	       	         // use the default name
  const db_path = `${WORKDB}/${db_file}`;

  /*---DELETE--*/
  if (options.delete) {
      if (!process.env.WORKBAK) {
          console.error('You must define a shell variable named WORKBAK as a backup directory before deleting a database file.');
          process.exit(1);
      }
      const db_path_bak = `${process.env.WORKBAK}/${db_file}.${Date.now()}`;
      fs.renameSync(db_path, db_path_bak);
      console.error(`Successfully deleted ${db_path};\nThis file has been backed up to ${db_path_bak}`);
      process.exit(0);
  }

  /*--ATTACH--*/
  const sqlite3 = require('sqlite3').verbose();      // remove 'verbose' in production
  const db = new sqlite3.Database(db_path, (err) => {
      if (err)
          console.error(`Error opening database file ${db_path}: ${err.message})`);
      else
          console.log(`Successfully attached to database file ${db_path}`);
  });
#+end_src

* Process CSV Files
:PROPERTIES:
:header-args: :comments both
:END:
The Node.js module [[https://csv.js.org/][~csv~]] contains both a [[https://csv.js.org/parse/][csv parser]] and [[https://csv.js.org/transform/][stream transformer]].
The pattern is to parse a CSV string into records and pipe those records
through the transformer to be massaged into shape.  From there the new data is
saved in an SQLite3 database using [[https://www.npmjs.com/package/sqlite3][~sqlite3~]] (see its [[https://github.com/mapbox/node-sqlite3/wiki/API][API]] also).

The processing of a CSV file, therefore, involves the following steps and
Node.js modules:

1. finding the correct CSV file (using ~FileSystem~)

2. reading the CSV records from the file as a string (using ~StreamReader~)

3. parsing the string into CSV records (using ~CSV-Parse~)

4. transforming the CSV records into better data (using ~CSV-Transform~)

5. saving the data in an SQLite database (using ~SQLite3~)

** Set Up Stream-Transform and Transform Function
:PROPERTIES:
:header-args: :comments both
:END:
This code implements the stream transformer functionality, which is at the
heart of this project.  The stream transformer receives data via its ~write~
method, executed in the parser for every CSV record in the file, in its
~transform~ method, which contains a transform function as its first argument.
This transform function argument performs the major work of manipulating each
record.  After it transforms the data, the transformer receives the new data
via a =readable= event, where it can process the data.  In this case, the data
will be saved into the SQLite3 database.

*** Set Up the Transform Function
The Transform Function receives a record and massages it into shape.

#+name:stream-transform-function
#+begin_src js +n :tangle index.js
  const accounting = require('accounting');

  const transform_function = function (record) {
      const DEBIT   = 'debit';
      const CREDIT  = 'credit';
      const CHECK   = 'check';
      const CASH    = 'cash';
      const UNKNOWN = 'unknown';
      const TXFR    = 'transfer';
      let   trfrom  = '';

      // Format date as yyyy-mm-dd; delete original Date
      record.date = new Date(record['Date']).toISOString().split('T')[0];
      delete record['Date'];

      // Change Transaction to type; delete original Transaction
      record.type = record['Transaction'].toLowerCase();
      delete record['Transaction'];

      // Change Amount to amount as Currency type; delete original Amount
      record.amount = accounting.formatMoney(record['Amount']);
      delete record['Amount'];

      // Change Name to payee; keep original Name as OrigName; delete Name
      record.payee = record['Name'].toLowerCase();
      record.OrigPayee = record['Name'];
      delete record['Name'];

      // Clean up Memo by removing Download message; return as memo; keep Memo as OrigMemo
      let re = new RegExp('Download from usbank.com.\\s*');
      record.memo = record['Memo'].replace(re,'').toLowerCase();
      record.OrigMemo = record['Memo'];
      delete record['Memo'];

      // Add new columns: check, txfr, _case, desc1, desc2
      record.check = null;
      record.txfr  = null;
      record._case = null;
      record.desc1 = null;
      record.desc2 = null;

      // Add check no. to check column
      if (record.payee === CHECK) {
          const checkno = record.type.replace(/^0*/,'');
          record.check = checkno;
          record.type = DEBIT;
          record.payee = UNKNOWN;
          record.memo += `Purchase by check no. ${checkno}`;
          record.desc1 = 'purchase';
          record.desc2 = 'check';
      }

      if (record.payee.match(/(internet|mobile) (banking) transfer (deposit|withdrawal) (\d{4})\s*$/)) {
          record.desc1 = RegExp.$3;
          record.desc2 = RegExp.$1;
          record.txfr = `${(RegExp.$3 === 'deposit') ? '<' : '>'} usb_${RegExp.$4}`;
          tofrom = (record.type === 'debit') ? 'to' : 'from';
          record.payee = (record.type === 'debit') ? `usb_${RegExp.$4}` : `usb_${options.csv[0]}`;
          record.memo = `${record.desc2} ${record.desc1}: ${TXFR} ${tofrom} ${record.memo}`;
      }

      if (record.payee.match(/debit (purchase)\s*-?\s*(visa)? /)) {
          record.desc1 = RegExp.$1;
          record.desc2 = RegExp.$2;
          record.payee = record.payee.replace(RegExp.lastMatch,'');
          record.memo = `${record.desc2} ${record.desc1} ${record.memo}`.trimLeft();;
      }

      if (record.payee.match(/(electronic) (withdrawal) |(web authorized) (pmt) |(atm|electronic) (deposit|withdrawal) /)) {
          tofrom = '';
          record.desc1 = RegExp.$2 ? RegExp.$2 : RegExp.$4 ? RegExp.$4 : RegExp.$6;
          record.desc2 = RegExp.$1 ? RegExp.$1 : RegExp.$3 ? RegExp.$3 : RegExp.$5;
          if (RegExp.$5 === 'atm' || RegExp.$5 === 'electronic') {
              record.payee = (RegExp.$6 === 'deposit') ? `usb_${options.csv[0]}` : CASH;
          } else {
              record.payee = record.payee.replace(RegExp.lastMatch,'');
          }
          if (record.memo.match(/paypal/) && record.type === CREDIT) {
              record.txfr = `< ${RegExp.lastMatch}`;
              tofrom = ' from';
          }
          record.memo = `${record.desc2} ${record.desc1}${tofrom} ${record.memo}`;
      }

      // if (record.payee.match(/\s*(portland|vancouver|seattle){2,}\s*(or|wa)\s*/)) {
      //     record.payee.replace(RegExp.lastMatch, `${RegExp.$1} ${RegExp.$2}`);
      //     record.memo.replace(RegExp.lastMatch, `${RegExp.$1} ${RegExp.$2}`);
      // }
      record.payee = record.payee.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.memo  = record.memo.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.payee = record.payee.replace(/\s*[-\d]{5,}\s*\w{2}$/,''); // '650-4724100 CA' & '        855-576-4493WA' & '  800-3333330 MA'
      record.memo  = record.memo.replace(/\s*[-\d]{5,}\s*\w{2}$/,'');
      record.payee = record.payee.replace(/(\s\w*https)?www.*$/,''); // WWW.ATT.COM TX; UDEMY ONLINE COUHTTPSWWW.UDECA
      record.memo  = record.memo.replace(/(\s\w*https)?www.*$/,'');
      record.payee = record.payee.replace(/\s*\w+\.com\s+\w{2}$/, '');
      record.memo  = record.memo.replace( /\s*\w+\.com\s+\w{2}$/, '');
      record.payee = record.payee.replace(/\s{2,}/g,' ');
      record.memo  = record.memo.replace(/\s{2,}/g,' ');

      return record;
  }
#+end_src

#+RESULTS: stream-transform-function
: undefined

*** Set Up the Stream Transform
#+name:stream-transformer
#+begin_src js +n :tangle index.js
  const csv = require('csv');
  const transformer = csv.transform(transform_function)
  const output = [];

  transformer.on('readable', function() {
      let record;
      while ((record = transformer.read())) {
          console.log(`Transformer record:\n${util.inspect(record)}`);
          output.push(record);
      }
  });

  transformer.on('error', function(err) {
      console.error(err.message);
  });

  transformer.on('finish', function() {
      console.log('Transformer finished writing records.');
  });

  transformer.on('end', function() {
      console.log('Transformer done reading records.');
  });
#+end_src

** Set Up CSV-Parse
:PROPERTIES:
:header-args: :comments both
:END:
This section implements the csv parser.  By default, it does little other than
read a large string of data and parse it into an array of records.  By giving
it the option =columns = true=, however, the parser will use the first line as
a list of column headings, and produce an array of objects where the keys are
column names, and the values are column entries.  Each record is written to the
stream transformer.

#+name:csv-sqlite3-csv-parse
#+header: :noweb yes
#+header: :comments link
#+begin_src js +n :tangle index.js
const parser = csv.parse({columns: true});
const records = [];

parser.on('readable', function() {
    console.log('Parser beginning to read records.');
    let record;
    while ((record = parser.read())) {
        console.log(`parser record:\n${util.inspect(record)}`);
        transformer.write(record);
    }
    transformer.end();
});

parser.on('error', function(err) {
    console.error(err.message);
});

parser.on('end', function() {
    console.log('Parser finished reading records.');
});

parser.on('finish', function () {
    console.log('Parser finished writing records.');
});
#+end_src

** Set Up StreamReader
This section implements the Stream Reader that reads the CSV file in as a large
string of data and sends it to the csv parser via the parser's ~write~ method.

CSV financial files are found in the directories =$WORKUSB_[6815|6831]/yyyy=,
where =yyyy= can be 2004--2019, and on.  Given =[6815|6831]= and a year
=[2004|2005...2019]=, the file path will be
=$WORKUSB_6815/YYYY/usb_6815--yyyy.csv=.  This code makes sure the file exists
and the user has proper permissions to read it before proceeding.

#+name:csv-sqlite3-process-csv-files
#+header: :noweb yes
#+begin_src js +n :tangle index.js
if (options.csv) {
    const acct = options.csv[0],
          year = options.csv[1];

    if (!process.env.WORKUSB) {
        console.error('You must assign a path to the shell variable WORKUSB');
        process.exit(1);
    }

    const acct_year_path = `${process.env.WORKUSB}/usb_${acct}/${year}`;
    const acct_year_csv_file = `usb_${acct}--${year}.csv`;
    const acct_year_csv_file_path = `${acct_year_path}/${acct_year_csv_file}`;
    if (!fs.existsSync(acct_year_csv_file_path) || !(fs.accessSync(acct_year_csv_file_path, fs.constants.R_OK) === undefined)) {
        console.error(`Cannot find or access the CSV file at '${acct_year_csv_file_path}'.`);
        process.exit(1);
    }
    console.log(`Successfully found the CSV file: '${acct_year_csv_file_path}'`);

    const csv_file_stream = fs.createReadStream(acct_year_csv_file_path, {encoding: 'utf8'});

    csv_file_stream.on('readable', function () {
        let record;
        while ((record = this.read())) {
            console.log(`readable record: ${record}`);
            parser.write(record);
        }
        parser.end();
    });

    csv_file_stream.on('error', function(err) {
        console.error(err.message);
    });

    csv_file_stream.on('end', function () {
        console.log('Reader finished reading data.');
    });
}
#+end_src

* Create Tables

* Index
:PROPERTIES:
:unnumbered: t
:index:    cp
:END:

* Macro Definitions                                                :noexport:
#+macro: heading @@texinfo:@heading @@$1
#+macro: subheading @@texinfo:@subheading @@$1

* Export Settings                                                  :noexport:
#+texinfo_filename:csv-sqlite3.info
#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:CSV
#+texinfo_dir_title:ConvertCSV (convertcsv)
#+texinfo_dir_desc:Convert USB CSV files to SQLite
#+texinfo_printed_title:ConvertCSV Using Node.js CSV-Parser

* Local Variables                                                  :noexport:
# Local Variables:
# time-stamp-pattern:"8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
