# -*- mode:org; fill-column:79; -*-
#+title: CSV-SQLite3 Using Node
#+subtitle:Convert CSV files into data used by SQLite3
#+subtitle:({{{version}}} {{{date(%A %B %d\, %Y)}}})
#+author:LOLH
#+date:<2019-10-20 Sun 22:13>
#+macro: version Version 0.0.88

#+texinfo: @insertcopying

* Problem Statement
:PROPERTIES:
:unnumbered: t
:END:
Instead of using Intuit Quicken or Microsoft Money, or even GnuCash, I would
like to use Ledger3 to manage my finances.  Two big problems immediately arise:

1. What do I do about the years of prior data that I have accumulated?

2. How do I efficiently process new data as it comes in from my bank?


The prior data comes in many different sizes and styles.  For the immediate
future, I will focus solely on the years 2016-2019.  I can add additional years
after getting things to work for 2016-2019.

New data (transactions) come in from my bank through CSV and QFX downloads.  I
will download and save both of the CSV and QFX data, but work exclusively with
the CSV files at this point.

There is a =Download Transactions= button that loads a modal window with
settings for

- Account
- From Date
- To Date
- Download To (i.e., format)


{{{noindent}}} So I can easily pick the account, start date, end date, and
format, download new transactions into a separate file.  The problem that
arises is that because this is a manual operation, it becomes difficult to keep
track of what dates have been downloaded and processed already, or even wanting
to reprocess some dates or all dates, etc. without accumulating duplicate data.
An acceptable solution is to concatenate newly downloaded data into files
according to account and year, then remove duplicate items.  This would be
accomplished with a Perl script concatenating the files, sorting the lines, and
unique'ifying them and then saving and backing up new and original files.

** The Workflow

A potential workflow would therefore be:

1. Download raw data from a bank by account, start date, and end date, which
   will be stored in the =~/Downloads= directory as ~export.csv~.  A script can
   be executed to review the data to determine what the year is.  However,
   there is no indication in either the filename or the data from which account
   the data was downloaded.  Therefore, any script that is run must be told
   what account this data is associated with, as for example here:

   : catuniq [--dest <path>] <acct>

   Invoking this command will look for a file name ~export.csv~ in the
   =~/Downloads= directory, determine what year its transactions are, and
   catenate the file onto the proper destination file according to year and
   account, and then sort and unique'ify the new file.  It will create the
   necessary destination directory and file to allow the command to run.

   The file onto which the export is catenated can either be indicated on the
   command line by means of the ~--dest~ option, or located through the
   environment variable =$WORKUSB= if the ~--dest~ option is omitted.  Backups
   are stored in $WORKBAK with the Unix time appended to the end.

   [[catuniq-source][Perl script catuniq.pl]]

2. Process the raw data to remove extraneous and unnecessary data while
   reformating the data into the proper form and columns for efficiently
   working with the financial data, e.g., producing accounting and tax reports,
   budgets, etc..  This processed data will be stored in an SQLite3 database,
   avoiding duplicating data at all times.

3. Export data from the SQLite3 database, and convert it into a form usable by
   the Ledger3 program, again always avoiding duplicating transactions.

4. Do as much as possible with the Ledger3 data automatically to create proper
   double entry transactions to avoid time-consuming manual work.


This workflow requires a way to download any amount of data and process it with
the click of a button without worrying about duplicating transactions.  Ledger3
is supposed to be able to detect duplicates and avoid reloading them with its
~convert~ command.  Therefore it would be convenient to be able to download any
amount of new data from a bank and have it combined with old data without
duplicating transactions as well.

Then, the workflow would process the downloaded data into a form usable in both
an SQLite3 database and Ledger3, again without having to worry about
duplicating data.  Ledger3 uses a hashing function to produce a unique hash for
each entry, similar to how ~git~ works.  This may be a possible solution for
the raw bank data as well.

** Source Code for catuniq.pl
   #+caption:Source code for catuniq.pl
   #+name:catuniq-source
   #+header: :shebang "#!/usr/bin/env perl"
   #+begin_src perl -n :tangle scripts/catuniq.pl :mkdirp yes

     package CATUNIQ;
     our $VERSION = "0.1.2";
     eval $VERSION;
     # 2019-07-31T12:30

     use strict;
     use warnings; no warnings 'experimental';
     use v5.16;

     use File::Spec;			# Portably perform operations on file names
     use File::Path;			# Create or remove directory trees
     use File::Slurp;		# Simple and Efficient Reading/Writing/Modifying of Complete Files
     use File::Copy;			# Copy files or filehandles
     use List::Util 'uniqstr';	# A selection of general-utility list subroutines
     use OptArgs2;			# Integrated argument and option processing
     use Data::Printer;		# colored pretty-print of Perl data structures and objects


     my @valid_accts = (qw'
         6815
         6831
         6151'
     );

     my @valid_years = (qw'
         2016
         2017
         2018
         2019'
     );


     # ESTABLISH THE CL OPTIONS AND ARGUMENTS
     opt help => (
         isa => 'Flag',
         comment => 'Help',
         alias => 'h',
         ishelp => 1,
     );

     arg acct => (
         isa     => 'Str',
         comment => 'The name of the account to which the file is related; e.g. "usb_6815" or "usb_6831"',
         required=> 1,
     );

     opt dest => (
         isa     => 'Str',
         alias   => 'd',
         comment => 'path to the destination file upon which the new data will be catenated.',
         default => exists $ENV{WORKUSB} ? $ENV{WORKUSB} : undef,
     );

     opt restore => (
         comment => 'Restore a backed-up file related to <ACCT>',
         isa     => 'Flag',
         alias   => 'r',
     );

     # PROCESS THE CL OPTIONS AND ARGUMENTS
     my $opts = optargs;




     # VERIFY FILES

     # Verify $WORKBAK exists
     exists $ENV{WORKBAK} || die("STOP: \$WORKBAK is not defined");

     # Verify correct form of 'acct', e.g., 'usb_6815'
     my ($acct) = $opts->{acct} =~ /usb_(\d{4})/ or die("STOP: incorrect acct form: $opts->{acct}");
     $acct ~~ @valid_accts or die("STOP: acct $acct is not a member of @valid_accts");

     # verify a $dest has been supplied
     die ("STOP: you did not supply a '-dest' option and \$WORKUSB is not defined.") unless exists $opts->{dest};
     my $dest = File::Spec->catdir($opts->{dest}, $opts->{acct}); # e.g., $WORKUSB/usb_6815

     if ($opts->{restore}) {
         say "Running restore.";
         restore();
         exit;
     }

     # Find and verify 'export.csv'
     my $export = File::Spec->catfile($ENV{HOME}, 'Downloads', 'export.csv');
     -e -r -w $export or die("STOP: $export must exist, be readable, and be writable.");

     # Find year from within export.csv
     my @lines = read_file($export);
     chomp(my $header = shift @lines); # remove the header line from $export
     my ($year) = $lines[1] =~ /([[:digit:]]{4})/
         or die("Cannot obtain a year from $export\n");

     # verify $dest_year dir exists or create it, including subdirectories
     my $dest_year = File::Spec->catdir($dest, $year); # e.g., $WORKUSB/usb_6815/2019
     File::Path::make_path($dest_year, {verbose => 1}) unless ( -d $dest_year );

     my $acct_year = "$opts->{acct}--${year}.csv";     # e.g., usb_6815--2019.csv
     my $dest_file = File::Spec->catfile($dest, $year, $acct_year); # e.g., $WORKUSB/usb_6815/2019/usb_6815--2019.csv




     # Backup original $dest_file to $WORKBAK before appending to
     my $dest_bak = File::Spec->catfile($ENV{WORKBAK}, "$acct_year." . time());
     copy($dest_file, $dest_bak);

     # APPEND $export onto $dest_file
     append_file($dest_file, @lines)
         or die("STOP: append of $dest_file and \@lines failed.\n");

     # UNIQUE new $dest_file
     @lines = uniqstr sort map { # first change date to year-mm-dd for proper sorting
         if (/([[:digit:]]{1,2})\/([[:digit:]]{1,2})\/([[:digit:]]{4})/) {
             my $year = sprintf("%4d-%02d-%02d",$3,$1,$2);
             s/$&/$year/;
         }
         $_;
     } read_file($dest_file);

     unshift @lines, pop @lines; # header ends up last after the sort; put it back to beginning

     # Save new $dest_file
     write_file($dest_file, @lines);

     # Backup export.csv to $WORKBAK
     move($export, File::Spec->catfile($ENV{WORKBAK}, "export.${acct_year}." . time()));



     say "SUCCESS: $export catuniq'ed onto $dest_file.";

     sub restore {
         use POSIX qw(strftime);
         my $acct = $opts->{acct}; # e.g. usb_6815
         my $dt = qr/^(\d{4}-\d{2}-\d{2})/;

         chdir $ENV{WORKBAK};
         opendir (my $dh, '.') || die "Can't open $ENV{WORKBAK}: $!";

         my @baks =
             sort { # sort by most recent backup first
                 my ($at) = $a->{t} =~ $dt; # just sort by datetime
                 my ($bt) = $b->{t} =~ $dt;
                 $bt cmp $at;
             }
             map { # change Unix time to POSIX ISO datetime %Y-%m-%dT%H:%M:%S
                 my ($acct, $time) = /^(.*.csv).(\d+)$/;
                 $time = substr $time, 0, 10; # remove milliseconds from those times that have them
                 my $t = (strftime "%F T %T", localtime($time)) . sprintf(" --- %s", $acct);
                 {t => $t, o => $_}; # map to POSIX time, and original filename as a hashref
             }
             grep {/$acct.*.csv/ }
             readdir($dh);

         foreach (@baks) {
             state $c = 0;
             printf("[%2d] %s\n", $c++, $_->{t});
         }

         print "Pick a number: ";
         chomp (my $num = <STDIN>);
         say "You chose $baks[$num]->{t} ($baks[$num]->{o})";
         print "restore (y/n)? ";
         exit unless <STDIN> =~ /y/i;

         my ($file) = $baks[$num]->{t} =~ /--- (.*)$/; # i.e., 'usb_6815--2019.csv'
         my ($year) = $file =~ /(\d{4})\.csv$/; # i.e., '2019'
         my $restore_path = File::Spec->catfile($dest,$year,$file); # full path to file to be restored
         my $bak_path = File::Spec->catfile($ENV{WORKBAK}, $baks[$num]->{o}); # full path to backed-up file

         # back up the file to be restored, just in case; use same directory
         move( ${restore_path}, ${restore_path}.'.bak') or die "Backup of ${restore_path} failed: $!";
         # note that the backed-up file will be deleted
         move( ${bak_path}, ${restore_path}) or die "Restore of $baks[$num]->{o} failed: $!";

         say "Successfully restored $baks[$num]->{o} to $restore_path";
     }
   #+end_src

   #+name:link-catuniq-into-workbin
   #+header: :results output :exports both
   #+begin_src sh
     ln -f $PWD/scripts/catuniq.pl $WORKBIN/catuniq
   #+end_src

   #+RESULTS: link-catuniq-into-workbin

* Introduction
:PROPERTIES:
:unnumbered: t
:END:
US Bank has the facility to download bank records in CSV form.  This program is
designed to convert those downloaded CSV files into a form usable by SQLite,
and then to use SQLite to process the data.

The USB table data columns and representative content coming from an exported
download is:

- Date :: ="9/3/2019"=
- Transaction :: ="DEBIT"= or ="CREDIT"=
- Name :: ="DEBIT PURCHASE -VISA COMCAST PORTLAND800-266-2278OR"=
- Memo :: ="Download from usbank.com. COMCAST PORTLAND800-266-2278OR"=
- Amount :: ="-81.0400"=


The converted USB table data (for both the SQLite3 DB and the CSV) should be:

- =rowid= :: (implicit creation)
- =acct= :: a String in the form of =usb_6815|usb_6831|usb_6151=
- =date= :: a Date in the form of =yyyy-mm-dd=
- =trans= :: an Enum containing either =credit | debit=
- =checkno= :: a String containing a check number, if present (=check= is a
               reserved word and throws an error)
- =txfr= :: a String containing a direction arrow (=<= or =>=) and a bank
            account (e.g. =usb_6151=)
- =payee= :: a String
- =category= :: a String
- =memo= :: a String
- =desc1= :: helpful information obtained from parsing
- =desc2= :: helpful information obtained from parsing
- =caseno= :: containing a related case number (=case= is a reserved word and
              throws an error)
- =amount= :: in the form =\pm##,###.##=
- =OrigPayee= :: stored for reference
- =OrigMemo= :: stored for reference


The following code is a =noweb= source, to be expanded into some code later.

#+caption:Define the USB Table Data Schema
#+name:define-usb-table-data-schema
#+begin_src sql
  usb (
         rowid     INTEGER PRIMARY KEY NOT NULL,
         acct      TEXT NOT NULL,
         date      TEXT NOT NULL,
         trans     TEXT NOT NULL,
         checkno   TEXT,
         txfr      TEXT,
         payee     TEXT NOT NULL,
         category  TEXT,
         note      TEXT,
         desc1     TEXT,
         desc2     TEXT,
         caseno    TEXT,
         amount    REAL NOT NULL,
         OrigPayee TEXT NOT NULL,
         OrigMemo  TEXT NOT NULL )
#+end_src

** SQLite3 USB Tables
#+cindex:tables, sqlite
#+cindex:sqlite tables
The minimum SQLite tables that should be created are:

- usb :: includes business (6815), trust (6831), personal (6151) data
- checks :: holds check information parsed from ~worklog.<year>.otl~ files
- cases :: 190301, etc.
- people :: John Doe, Mary Jane, etc.
- businesses :: Law Office of ..., etc.


More can be created as needed.

#+cindex:@file{db_data.js}
#+cindex:db_data
#+cindex:DB Data table info
#+caption:Define SQLite DB Data
#+name:define-sqlite-db-data
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/sql/db_data.js
  /* sql/data */

  exports.DB_TABLES = {
      usb:    'usb',
      checks: 'checks',
  };

  exports.DB_ACCTS = {
      '6815': 'Business',
      '6831': 'Trust',
      '6151': 'Personal',
  };

  exports.DB_YEARS = [
      '2016',
      '2017',
      '2018',
  ];

  exports.DB_COLS = [
      'acct',
      'date',
      'trans',
      'checkno',
      'txfr',
      'payee',
      'category',
      'note',
      'desc1',
      'desc2',
      'caseno',
      'amount',
      'OrigPayee',
      'OrigMemo',
  ];

  exports.EXPORT_DB_COLS = [
      'rowid',
      'acct',
      'date',
      'trans',
      'checkno',
      'txfr',
      'payee',
      'category',
      'note',
      'caseno',
      'amount',
  ];

#+end_src

** SQLite3 Checks Table

#+cindex:Checks table
#+cindex:table, checks
There will also be a Checks table which will hold check data parsed from the
=WORKLOG= files.  See [[Find and Store Checks]].

The column names for the =checks= table columns are:

- =rowid= :: implicit creation
- =acct=  :: Enum of =6815 | 6831 | 6151=
- =checkno= :: String, e.g., 1001, 1002
- =date= :: Date =yyyy-mm-dd=
- =payee= :: String
- =subject= :: String
- =purpose= :: String
- =caseno= :: e.g., 190301, 190205
- =amount= :: =\pm$#,###.##=


The following is another =noweb= data source.

#+caption:Define Checks Table Schema
#+name:define-checks-table-schema
#+begin_src sql
  checks (
          acct		TEXT NOT NULL,
          checkno		TEXT NOT NULL,
          date		TEXT NOT NULL,
          payee		TEXT NOT NULL,
          subject		TEXT NOT NULL,
          purpose		TEXT,
          caseno		TEXT NOT NULL,
          amount		REAL NOT NULL
  )
#+end_src

#+cindex:@file{db_data.js}
#+caption:Define SQLite3 Checks Data
#+name:define-sqlite3-checks-data
#+begin_src js +n :tangle lib/sql/db_data.js
  exports.CHECKS_COLS = [
      'acct',
      'checkno',
      'date',
      'payee',
      'subject',
      'purpose',
      'caseno',
      'amount'
  ];
#+end_src

** The Environment
This program, and in fact all =Worklog= programs, require depend upon a certain
environment being present.  This module returns that environment after checking
for the existence of the dependencies.

#+caption:Check and Return the Environment
#+name:check-and-return-the-environment
#+header: :mkdirp yes
#+begin_src js -n :tangle config/env.js
  /* config/env */

  exports.WORK    = process.env.WORK;
  exports.WORKDB  = process.env.WORKDB;
  exports.WORKFIN = process.env.WORKFIN;
  exports.WORKUSB = process.env.WORKUSB;
  exports.WORKCSV = process.env.WORKCSV;
  exports.WORKBAK = process.env.WORKBAK;
  exports.WORKNODE=process.env.WORKNODE;
  exports.WORKLEDGER = process.env.WORKLEDGER;

  (function check() {
      let good = true;
      for (env in exports) {
          if (typeof exports[env] === 'undefined') {
              console.error(`ERROR: Environment variable ${env} is not defined.`);
              good = false;
          }
      }
      if (!good) {
          console.error('Halting execution');
          process.exit(1);
      }
  })();
#+end_src

* Create the Project

Creating the project involves extracting all of the files (tangling),
installing a ~package.json~ file, identifying and importing dependency
packages, and creating an entry point for the project's use, an ~index.js~
file.

** Usage Information

: node csv-sqlite3 --csv ACCT YEAR  [--attach [DB]]

: node csv-sqlite3 --delete [DB]

: node csv-sqlite3 --export ACCT YEAR [--attach [DB]]

The first version identifies a CSV source file to import into an SQLite3
database.  The only required information is the ACCT and YEAR of the CSV file.
If nothing else is given, the operation is assumed to be {{{option(--attach)}}}
and the name of the database defaults to ~workfin.sqlite3~.

The second version deletes the SQLite3 database file and any exported CSV
files.

The third version exports data from the SQLite3 database into a CSV file of the
same name as the database file, and converts the data into Ledger format and
saves it as a separate Ledger file.  The name of the database to export from is
assumed to be the default unless the {{{option(--attach)}}} option is given
with a different name.

** Extract the Modules

The following code runs first to extract (``tangle'') all file modules into
their respective directories.  This command runs automatically upon this source
file being woven into documentation using Org's =export= facilities.  This
command can be run independently from within the Org source file using the key
command =C-c C-v t=, or from the command line using the Make target =make
tangle=.

Note that in order for it to run automatically and without user confirmation,
the Org variable =org-confirm-babel-evaluate= must be set to =nil=, either
through the customization feature, in an =init= file, or as a local variable in
this buffer.

#+caption:Org Babel command to extract (``tangle'') all modules
#+name:tangle-CSV-SQLite3-project
#+begin_src emacs-lisp :results output :exports both
(org-babel-tangle-file "CSV-SQLite3.org")
#+end_src

** Helper Scripts

Two helper scripts are also created by this source file:

- ~catuniq~ :: imports newly exported data from US Bank into the proper CSV
               source file.  The two required options are the USB account to
               catenate unto, e.g., =usb_6815=, and the destination path, which
               can be omitted if the environment variable =WORKUSB= is defined
               pointing to the source USB CSV files.  Files exported from US
               Bank have the path: ~$HOME/Downloads/export.csv~.

               : catuniq --acct ACCT [--dest PATH]

- ~find-checks~ :: parses a WORKLOG file of a particular year for check
                   information and stores it in the SQLite3 database in a
                   =checks= table.

                   : find-checks YEAR

** Install Package.json and the Package's Dependencies

The package's ~package.json~ file is created using Yarn, which is also used to
install external dependencies.

- ~command-line-ars~
- ~command-line-usage~
- ~csv~
- ~sqlite3~
- ~accounting~
- ~wlparser~

#+caption:Create the CSV-SQLite3 Package
#+name:create-CSV-SQLite3-package
#+header: :results output :exports both
#+begin_src sh
  yarn --yes --private init
  sed -i '' -e 's/\"version\": \"1.0.0\",/\"version\": \"0.0.76\",/' package.json
  yarn add command-line-args command-line-usage csv sqlite3 accounting
  yarn add ssh://git@github.com:wlharvey4/wlparser.git#prod
#+end_src

#+caption:Package json
#+name:csv-sqlite3-package.json
#+BEGIN_SRC sh :results output :exports both
cat package.json
#+END_SRC

** Create the Index.js File

The entry point to the project is through its ~index.js~ file.  This file
processes the command-line arguments and dispatches to the appropriate points,

#+name:csv-sqlite3-dependencies
#+header: :mkdirp yes
#+begin_src js -n :tangle index.js
/* index.js */

const cl_args  = require('command-line-args');
const cl_usage = require('command-line-usage');
const ENV = require('./config/env');

#+end_src

** Working with the Command Line

The Node.js command-line programs ~command-line-args~ and ~command-line-usage~
parse the arguments and dispatch to the appropriate modules and provide usage
information.

*** Command Line Usage
#+cindex:command-line usage
#+cindex:usage
#+cindex:@command{command-line-usage}
#+cindex:@code{cl_usage()}
This section generates a usage message activated by the =--help= option.  It
uses the [[option-defs-variable][~option_defs~]] object.

{{{heading(Options.defs)}}}

<<option-defs-variable>>
#+caption:The Option Defs object
#+name:option-defs-object
#+begin_src js +n :tangle index.js
  const option_defs = [
      { name: 'help',   alias: 'h', type: Boolean, description: 'Prints this usage message.' },
      { name: 'attach', alias: 'a', type: String,  description: 'Attach to an existing or new database file.' },
      { name: 'delete', alias: 'd', type: String,  description: 'Delete an existing database file and related CSV files.' },
      { name: 'csv',    alias: 'c', type: String,  description: 'Process a CSV file [6815|6831] yyyy', multiple: true  },
      { name: 'export', alias: 'e', type: String,  description: 'Export identified sqlite3 data into a csv file', multiple: true },
      { name: 'checks',             type: Boolean, description: 'Find checks in worklog.<year>.otl and save to sqlite3 database checks file.' },
      { name: 'log-level', alias: 'l', type: Number, description: 'Set a log level 0..10' },
  ];

  const usage = [
      {
          header: 'CSV-SQLite3',
          content: 'Processes raw usb csv files into a form usable by SQLite3'
      },
      {
          header: 'Options',
          optionList: option_defs,
      },
      {
          content: `Project directory: {underline ${process.env.WORKNODE}/CSV-SQLite3}`
      }
  ];

#+end_src

*** Command Line Argument Processing
#+cindex:command-line arguments
#+cindex:arguments
#+cindex:@command{command-line-arguments}
#+cindex:@option{--help}
#+cindex:@option{--create}
#+cindex:@option{--delete}
Options include giving the name of a database to attach to using =--attach
<db>=.  In the absence of this option, a default database will be used.  A
database can be deleted here as well using the option =--delete <db>=, with a
backup being saved in the =WORKBAK= directory wtih the unix time suffixed to
the end.  If the option {{{option(--checks)}}} is included, parse the
~worklog.<year>.otl~ worklog file for check information and save the same in a
=checks= table in =<db>=.

{{{subheading(Usage)}}}

Obtain usage information using the {{{option(--help)}}} option:

: csv-sqlite3 --help | -h

{{{subheading(Choose the CSV File)}}}

For non-delete commands, identify the CSV file to transform via the =--csv=
option:

: csv-sqlite3 --csv | -c 6815|6831 2004...2019

{{{subheading(Attach and Delete a Database)}}}

The database is attachable (it will be created automatically if it does not
exist), and deleteable; this option need only be used if a non-standard
database name is used.  The default name is ~workfin.sqlite~.

: csv-sqlite3 --attach | -a <db-name>

: csv-sqlite3 --delete | -d <db-name>

{{{subheading(Export the SQLite3 data to CSV)}}}

Export SQLite3 data identified by <ACCT> and <YEAR> into a CSV file of the same
database name; use {{{option(--attach)}}} to identify a non-default database.

: csv-sqlite3 --export | -e <ACCT> <YEAR>

{{{subheading(Set a Log Level)}}}

: csv-sqlite3 --log-level <value:0..10> | -l <value:0..10>

** Main ~index.js~ Implementation

#+name:csv-sqlite3-command-line-arg-processing
#+header: :noweb yes
#+begin_src js +n :tangle index.js
  const options = cl_args(option_defs);
  console.log(options);

  if (options.help) {
      console.log(cl_usage(usage));
      process.exit(0);
  }

  let LOG_LEVEL = process.env.LOG_LEVEL || 1;
  if (options['log-level'] >= 0) {
      if (typeof options['log-level'] === 'number' && options['log-level'] <= 10)
          LOG_LEVEL = options['log-level'];
      else {
          console.error(`Incorrect log-level: ${options['log-level']}; must be between 0 and 10`);
      }
  }
  console.log(`Log-level set at: ${LOG_LEVEL}`);

  /* DELETE */
  if ( options.hasOwnProperty('delete') ) {
      require('./lib/sql/db').delete(options.delete);
      process.exit(0);
  }

  if ( !options.hasOwnProperty('attach') ) {
      options.attach = null;
  }

  if ( options.hasOwnProperty('csv') ) { // process --csv ACCT YEAR
      // options.csv[0] == acct
      // options.csv[1] == year

      console.log('Entered --csv');

      /* send a source stream through the file parser */
      const csv_file_stream   = require('./lib/csv/source_stream')(options.csv);
      const csv_file_parser   = require('./lib/csv/file_parser')(options);

      csv_file_stream.on('error', function(err) {
          console.error(err.message);
          process.exit(1);

      }).on('end', function () {
          console.log('Reader finished reading data.');

      });

      console.log('Ready for the rabbit hole');
      process.exit(0);
      csv_file_stream.pipe(csv_file_parser); // down the rabbit hole
  }
#+end_src

* Attach To or Delete a Database

SQLite3 can work with any number of databases.  Only one is initially attached,
but more can be attached subsequent to the first attachment.  When a database
is first opened it will be created if it does not exist.  In this program, if
the user requests that a database file be deleted, it will be deleted by
sending it to a backup directory (identified by the environment variable
=WORKBAK=) with the unix time stamp at the time of its deletion attached to the
end of the filename.  This program also supplies a way to restore a backed-up
database that was deleted.

The user can =attach= to a database file (either a specified file or the
default file, defined as ~$WORKFIN/db/workfin.sqlite~), or =delete= a specified
database file and the associated CSV file exported from the database data.  All
deleted files are backed up to a backup directory as specified above.

#+cindex:@file{db} database file
#+cindex:database file @file{db}
The attached database will be referenced as ~db~.

{{{heading(Verbose Mode)}}}

#+cindex:verbose mode
During development, call the ~verbose()~ method on the ~sqlite3~ object to
enable better stack traces.  In production, remove this call for improved
performance.

#+caption:Create the SQLite3 Database
#+name:create-sqlite3-database
#+header: :mkdirp yes
#+header: :noweb yes
#+begin_src js -n :tangle lib/sql/db.js
  /* lib/sql/db */

  const fs   = require('fs');
  const path = require('path');
  const ENV  = require('../../config/env');
  const WORKDB = ENV.WORKDB;      // base directory for storing workfin sqlite databases
  if (!fs.existsSync(WORKDB)) fs.mkdirSync(WORKDB);
  const WORKCSV = ENV.WORKCSV;       // base directory for .csv files
  if (!fs.existsSync(WORKCSV)) fs.mkdirSync(WORKCSV);
  const WORKLEDGER = ENV.WORKLEDGER; // base directory for .ledger files
  if (!fs.existsSync(WORKLEDGER)) fs.mkdirSync(WORKLEDGER);

  const DB_DEFAULT = 'workfin';  // default sqlite db name
  const EXT = '.sqlite';

  console.error('In db.js');

#+end_src

** DB Attach Code

#+caption:DB Attach
#+name:db-attach
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/sql/db.js

  /*--ATTACH--*/
  exports.attach = function (db_file) {

      const {db_file_base, db_path} = format_paths(db_file);
      console.log(`Attaching db_file: ${db_file_base}`);

      const sqlite3  = require('sqlite3').verbose();      // remove 'verbose' in production

      const db = new sqlite3.Database(db_path, err => {
          if (err) {
              return console.error(`Error opening database file ${db_path}: ${err.message})`);
          }
          console.log(`Successfully attached to database file ${db_path}`);
      });
    
      db.serialize();
      db.run(`CREATE TABLE IF NOT EXISTS
      <<define-usb-table-data-schema>>`);

      return db;
  }
#+end_src

** DB Delete Code

#+caption:DB Delete
#+name:db-delete
#+header: :mkdirp yes
#+begin_src js +n :tangle lib/sql/db.js

  /*---DELETE--*/
  exports.delete = function (db_file) {

      const {db_file_base, db_path, csv_path} = format_paths(db_file);

      const WORKBAK = ENV.WORKBAK;       // base directory for storing deleted files
      if (!fs.existsSync(WORKBAK)) fs.mkdirSync(WORKBAK);

      const WORKBAK_DB = path.format({
          dir: WORKBAK,
          name: 'db'
      });
      if (!fs.existsSync(WORKBAK_DB)) {
          fs.mkdirSync(WORKBAK_DB, {recursive: true});
      }

      const WORKBAK_CSV= path.format({
          dir:   WORKBAK,
          name: 'csv'
      });
      if (!fs.existsSync(WORKBAK_CSV)) {
          fs.mkdirSync(WORKBAK_CSV, {recursive: true});
      }

      const WORKBAK_LEDGER = path.format({
          dir:   WORKBAK,
          name: 'ledger'
      });
      if (!fs.existsSync(WORKBAK_LEDGER)) {
          fs.mkdirSync(WORKBAK_LEDGER, {recursive: true});
      }

      // Backup DB.sqlite, DB.csv
      const db_path_bak = path.format({
          dir: WORKBAK_DB,
          name: db_file_base,
          ext: `${EXT}.${Date.now()}`
      });

      const csv_path_bak = path.format({
          dir: WORKBAK_CSV,
          name: db_file_base,
          ext: `.csv.${Date.now()}`
      });

      try {
          fs.renameSync(db_path, db_path_bak);
          console.error(`Renamed ${db_path} to ${db_path_bak}`);
          fs.renameSync(csv_path, csv_path_bak);
          console.error(`Renamed ${csv_path} to ${csv_path_bak}`);
      } catch (err) {
          if (err.code === 'ENOENT')
              console.log(`file ${db_path} and/or ${csv_path} did not exist; ignoring.`);
          else {
              throw err;
          }
      }

      // Backup all .csv files
      try {
          const files = fs.readdirSync(WORKCSV);
          files.forEach(file => {
              const db_csv_path_file = path.format({
                  dir: WORKCSV,
                  name: file
              });
              const db_csv_path_bak  = path.format({
                  dir: WORKBAK_CSV,
                  name: file,
                  ext: `.${Date.now()}`
              });
              fs.renameSync(db_csv_path_file, db_csv_path_bak);
              console.log(`Renamed ${db_csv_path_file} to ${db_csv_path_bak}`);
          });

      } catch (err) {
          if (err.code === 'ENOENT') {
              console.log(`${db_csv_path} probably does not exist`);
          } else {
              throw err;
          }
      }

      /* Ledger */
      try {
          const files = fs.readdirSync(WORKLEDGER);
          files.forEach(file => {
              if (!/zero/.test(file)) { // don't backup the zero ledger file
                  const ledger_file = path.format({
                      dir: WORKLEDGER,
                      name: file
                  });
                  const ledger_file_bak = path.format({
                      dir: WORKBAK_LEDGER,
                      name: file,
                      ext: `.${Date.now()}`
                  });
                  fs.renameSync(ledger_file, ledger_file_bak);
                  console.log(`Renamed ${ledger_file} to ${ledger_file_bak}`);
              }
          });

      } catch (err) {
          if (err.code === 'ENOENT') {
              console.log(`${ledger_path} probably does not exist`);
          } else {
              throw err;
          }
      }
  }

  function format_paths (db_file) {

      if (db_file === null) {
          db_file = DB_DEFAULT;
      }

      const ext = path.extname(db_file);

      if (! (ext  === '' || ext === EXT)) {
          console.error(`ERROR: wrong extension: ${ext}.  Either leave it off or use ${EXT}.`);
          process.exit(1);
      }
      db_file_base = path.basename(db_file, EXT);

      const db_path = path.format({
          dir: WORKDB,
          name: db_file_base,
          ext:  EXT
      });

      const csv_path = path.format({
          dir: WORKCSV,
          name: db_file_base,
          ext: '.csv'
      });

      return ({db_file_base, db_path, csv_path});
  }
#+end_src

* Process CSV Files using the Node.js Package ~csv~

The Node.js package [[https://github.com/adaltas/node-csv][~node-csv~]] contains the following components (see
https://csv.js.org):

- [[https://csv.js.org/parse/][csv-parse]],

- [[https://csv.js.org/transform/][csv-transform]],

- [[https://csv.js.org/stringify/][csv-stringify]],


{{{noindent}}} all of which will be used in this project.

{{{heading(Objective is to Transform CSV Bank Records into Ledger Records)}}}

The objective is to open a CSV file as a stream, parse the file into CSV
records, pipe those records through the transformer to be massaged into shape,
and save the parsed and transformed records in an SQLite3 database using the
Node.js package [[https://www.npmjs.com/package/sqlite3][~sqlite3~]] (see its [[https://github.com/mapbox/node-sqlite3/wiki/API][API]] also).  After the bank records have been
transformed and placed into an SQLite database, they can be exported and sent
through a Ledger converter to be saved as Ledger entries.

{{{heading(Conversion)}}}

The conversion process attempts to turn a single entry accounting system into a
double entry accounting system.  There are quite a number of programs that
attempt to make this process as painless as possible, but I have not had an
opportunity to vet any of them except Ledger's own, the {{{command(convert}}}
command.  At this point, that is the one this project uses, but I plan on
looking at some of the others in the future.

{{{heading(Processing CSV Files)}}}

The processing of a CSV file, therefore, involves the following steps and
Node.js modules:

1. Find the correct CSV file (using ~FileSystem~) and open it
   as a Node.js [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_readable_streams][ReadableStream]] object, which implements the interface defined
   by the [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_class_stream_readable][~stream.Readable~ class]].

  - [[*Set Up the CSV Source Stream][Set Up the CSV Source Stream]]

2. Open a new CSV file to hold the new transformed data as a [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_writable_streams][Writable Stream]]

   - [[*Set Up CSV-Stringify][Set Up CSV-Stringify]]

3. Open an SQLite3 database to hold the new transformed data

   - [[*Attach To or Delete a Database][Attach To or Delete a Database]]

4. Read the CSV records from the file as a string (using ~StreamReader~)

   - [[*Set Up the CSV Source Stream][Set Up the CSV Source Stream]]

5. Parse the string into JS records (using ~CSV-Parse~)

   - [[*Set Up CSV-Parse][Set Up CSV-Parse]]

6. Transform the JS records into usable data (using ~CSV-Transform~)

   - [[*Set Up the CSV File Transform][Set Up the CSV File Transform]]

   - [[*Set Up the Transform Function][Set Up the Transform Function]]

7. Save the new data in the new CSV file (using ~StreamWriter~)

   - [[*Set Up CSV-Stringify][Set Up CSV-Stringify]]

8. Save the new data in an SQLite3 database (using ~SQLite3~)

   - [[*Set Up the CSV File Transform][Set Up the CSV File Transform]]

** Set Up the CSV Source Stream
This section implements a =Readable Stream= that reads the CSV file in as a
string of data and sends it to a CSV ~parser~ via the parser's ~write()~
method.

{{{heading(CSV Financial Files)}}}

CSV financial files are found in the set of directories 
: =$WORKUSB/usb_[6815|6831|6151]/yyyy=,

{{{noindent}}}where =yyyy= can be one of 2004--2019.  Given an account
(=[6815|6831|6151]=) and a year (=[2004|2005...2019]=), the source path is
constructed as:
: =$WORKUSB/usb_ACCT/YYYY/usb_ACCT--yyyy.csv=.

{{{heading(The {{{option(--csv)}}} Option)}}}

The user supplies an account (=6815|6831|6151=) and a year (one of
=[2016|2017|2018|2019]=) using the {{{option(--csv)}}} option on the command
line, e.g.:
: --csv 6815 2016

{{{noindent}}}Given this data, a source CSV file can be found, validated by
making sure that (1) the file exists and (2) the user has proper permissions to
read it before proceeding, and then turned into a =Readable Stream=.  Once a
=Readable Stream= has been obtained, it can be sent to the CSV parser via the
parser's ~write()~ method.

{{{heading(CSV Source Stream)}}}

#+caption:Create the CSV Source Stream
#+name:create-csv-source-stream
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/csv/source_stream.js
    /* csv/source_stream */

  console.log('Entered source_stream.js');

  const fs   = require('fs');
  const path = require('path');
  const {WORKUSB} = require('../../config/env');

  module.exports = function(csv_info) { // csv_info = options.csv = [acct, year]

      console.log('With csv_info: ', csv_info);

      try {
          return fs.createReadStream(
              validate_data(csv_info),
              {encoding: 'utf8'}
          );

      } catch (err) {
          console.error(err.message);
          process.exit(1);
      }
  }

  function validate_data ([acct, year]) {
      /* 'acct' and 'year' come from the command-line option '--csv'
         acct = options.csv[0]
         year = options.csv[1] */

      const csv_path = path.join(
          WORKUSB,
          `usb_${acct}`,
          year,
          `usb_${acct}--${year}.csv`
      );

      if (fs.existsSync(csv_path)) {
          try {
              (fs.accessSync(csv_path, fs.constants.R_OK));
              return csv_path;

          } catch (err) {
              console.error(err.message);
              throw err;
          }

      } else {
          throw new ReferenceError(`The path ${csv_path} does not exist.`);
      }
  }
  #+end_src

** Set Up CSV-Parse

#+cindex:@code{write} method, transformer
This section implements the csv parser.  By default, it does little other than
read a large string of data and parse it into an array of records.  By giving
it the option =columns = true=, however, the parser will use the first line as
a list of column headings, and produce an array of objects where the keys are
column names, and the values are column entries.  Each record is written to the
stream transformer using its =WRITE= method.

<<csv-transformer-write-method>>

#+caption:Create the CSV File Parser
#+name:create-csv-file-parser
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/csv/file_parser.js
  /* csv/file_parser */

  console.log('Entered file_parser');
  const parser      = require('csv').parse({columns: true});

  module.exports = function (options) {
    
      const transformer = require('./file_transformer')(options);

      parser.on('readable', function() {
          let record;
          while (record = parser.read()) {
              transformer.write(record); // continue down the rabbit hole
          }
      }).on('error', function(err) {
          console.error(err.message); 

      }).on('end', function() {
          console.log('Parser has ended reading.');

      }).on('finish', function() {
          console.log('Parser finished writing.');
          transformer.end();

      });
  }
#+end_src

** Set Up CSV-Transform and the Transform Function

This code implements the stream transformer functionality, which is at the
heart of this project.

The Transformer is a [[https://nodejs.org/dist/latest-v12.x/docs/api/stream.html#stream_class_stream_transform][Node.js Transform Stream]].  This means it is capable of
both reading and writing data.  In this project, the CSV Parser gets a
reference to the Transformer and [[csv-transformer-write-method%0A][writes parsed data]] to it; the Transformer
receives this data via its ~transformer.read()~ method.  This ~transformer~
object has a ~transform()~ method that takes a function callback, whose purpose
is to to /transform/ records that are read.  This is the heart of this project.

#+cindex:@code{transform()} function
The ~transform()~ function is implemented in the following section, and returns
completely transformed CSV bank records at its end.  These transformed records
are then written to both a new CSV file, and the SQLite3 database via the CSV
Stringifier object.

#+attr_texinfo: :options CSV transform ( transform_callback )
#+begin_defmethod
The CSV ~transform~ method reads a record and sends that record to a
=TRANSFORM_CALLBACK= that is used to /transform/ the data.
#+end_defmethod

After it transforms the data, the transformer receives the new data via a
=readable= event, where it can process the data.

#+cindex:@code{INSERT} into @file{db}
#+cindex:@command{db.run}
The transformed data will be saved into the SQLite3 database via an =INSERT=
statement executed by the ~db.run()~ method.

*** Set Up the Transform Function
The Transform Function receives a record and massages it into shape.  The
following regular expressions were created based upon inspection of the raw
data as it came from the bank for years 2016, 2017, and 2018.  It does a decent
job of creating readable payees and memos, as well as txfrs (transfers), but it
has not been set up to do anything for check payees, categories or related
records, for example.

#+caption:The CSV Transform Callback Function
#+name:stream-transform-function
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/csv/transform_function.js
  module.exports = function (record) {
      const DEBIT   = 'debit';
      const CREDIT  = 'credit';
      const CHECK   = 'check';
      const CASH    = 'cash';
      const DEPOSIT = 'deposit';
      const UNKNOWN = 'unknown';
      const TRANS   = 'transfer';
      const USBANK  = 'usbank';
      let   trfrom  = '';

      // Add new columns: acct, checkno, txfr, caseno, desc1, desc2, category
      record.acct    = usb_acct;
      record.checkno = null; // check no.
      record.txfr    = null; // direction and acct #
      record.caseno  = null; // related case foreign key
      record.desc1   = null; // noun
      record.desc2   = null; // adjective
      record.category= null; // categorization of the transaction

      // Format date as yyyy-mm-dd; delete original Date
      record.date = new Date(record['Date']).toISOString().split('T')[0];
      delete record['Date'];

      // Change Transaction to trans; delete original Transaction
      record.trans = record['Transaction'].toLowerCase();
      delete record['Transaction'];

      // Change Amount to amount as Currency type; delete original Amount
      record.amount = accounting.formatMoney(record['Amount']);
      delete record['Amount'];

      // Change Name to payee; keep original Name as OrigName; delete Name
      record.payee = record['Name'].toLowerCase().trimRight();
      record.OrigPayee = record['Name'];
      delete record['Name'];

      // Clean up Memo by removing Download message; return as note; keep Memo as OrigMemo
      let re = new RegExp('Download from usbank.com.\\s*');
      record.note = record['Memo'].replace(re,'').toLowerCase();
      record.OrigMemo = record['Memo'];
      delete record['Memo'];

      // Add check no. to checkno column
      if (record.payee === CHECK) {
          const checkno = record.trans.replace(/^0*/,'');
          record.checkno  = checkno;
          record.trans   = DEBIT;
          record.payee  = `(${record.checkno}) check`;
          record.note  += `Purchase by check no. ${checkno}`;
          record.desc1  = 'purchase';
          record.desc2  = 'check';
      }

      if (record.payee.match(/(returned) (item)/)) {
          record.desc1 = RegExp.$2;
          record.desc2 = RegExp.$1;
          record.payee = USBANK;
          record.note = `${record.desc2} ${record.desc1}`;
      }

      if (record.payee.match(/(internet|mobile) (banking) transfer (deposit|withdrawal) (\d{4})\s*$/)) {
          record.desc1 = RegExp.$3;
          record.desc2 = RegExp.$1;
          record.txfr = `${(RegExp.$3 === 'deposit') ? '<' : '>'} usb_${RegExp.$4}`;
          tofrom = (record.trans === 'debit') ? 'to' : 'from';
          record.payee = (record.trans === 'debit') ? `usb_${RegExp.$4}` : `usb_${options.csv[0]}`;
          record.note = `${record.desc2} ${record.desc1}: ${TRANS} ${tofrom} ${record.note}`;
          if (/>/.test(record.txfr)) {
              record.payee = `Transfer to ${record.payee} from ${record.acct}`;
          } else {
              record.payee = `Transfer to ${record.payee} from usb_${RegExp.$4}`;
          }
      }

      if (record.payee.match(/debit (purchase)\s*-?\s*(visa)? /)) {
          record.desc1 = RegExp.$1;
          record.desc2 = RegExp.$2;
          record.payee = record.payee.replace(RegExp.lastMatch,'');
          record.note = `${record.desc2} ${record.desc1} ${record.note}`.trimLeft();;
      }

      // Removed ELECTRONIC WITHDRAWAL for payment to State Bar of CA
      if (record.payee.match(/^.*(state bar of ca)/)) {
          record.payee = RegExp.$1;
      }

      // web authorized payment
      // atm|electronic|mobile check|rdc deposit|withdrawal <name>
      if (record.payee.match(/(web authorized) (pmt) |(atm|electronic|mobile)?\s*(check|rdc)?\s*(deposit|withdrawal)\s*(.*)?/)) {
          tofrom = '';
          record.desc1 = RegExp.$2 ? RegExp.$2 : RegExp.$4 ? RegExp.$4 : RegExp.$5 ? RegExp.$5 : 'undefined';
          record.desc2 = RegExp.$1 ? RegExp.$1 : RegExp.$3 ? RegExp.$3 : 'undefined';
          if (RegExp.$3 === 'atm' || RegExp.$3 === 'electronic' || RegExp.$3 === 'mobile' || RegExp.$5 === DEPOSIT) {
              record.payee = (RegExp.$5 === 'deposit') ? `usb_${options.csv[0]}` : CASH;
          } else {
              record.payee = record.payee.replace(RegExp.lastMatch,'');
          }
          if (record.note.match(/paypal/) && record.trans === CREDIT) {
              record.txfr = `< ${RegExp.lastMatch}`;
              tofrom = ' from';
          }
          record.note = `${record.desc2} ${record.desc1}${tofrom} ${record.note}`.trimRight();
      }

      if (record.payee.match(/(zelle instant) (pmt) (from (\w+\s\w+))\s(.*)$/)) {
          record.desc1 = RegExp.$2;
          record.desc2 = RegExp.$1;
          record.note = `${record.desc2} ${record.desc1} ${RegExp.$3}`;
          record.payee = `usb_${options.csv[0]}`;
      }

      if (record.payee.match(/(overdraft|international) (paid|processing) (fee)/)) {
          record.desc1 = RegExp.$3;
          record.desc2 = `${RegExp.$1} ${RegExp.$2}`;
          record.payee = USBANK;
          record.note  = `${record.desc2} ${record.desc1} to ${record.payee}`;
      }

      record.payee = record.payee.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.note  = record.note.replace(/\s*portland\s{2,}or$|\s*vancouver\s{2,}wa.*$/,'');
      record.payee = record.payee.replace(/\s\d{3}\w+\s{2,}or$/,''); // Nike Company 019Beaverton   OR
      record.note  = record.note.replace(/\s\d{3}\w+\s{2,}or$/,'');
      record.payee = record.payee.replace(/\s*[-\d]{5,}\s*\w{2}$/,''); // '650-4724100 CA' & '        855-576-4493WA' & '  800-3333330 MA'
      record.note  = record.note.replace(/\s*[-\d]{5,}\s*\w{2}$/,'');
      record.payee = record.payee.replace(/(\s\w*https)?www.*$/,''); // WWW.ATT.COM TX; UDEMY ONLINE COUHTTPSWWW.UDECA
      record.note  = record.note.replace(/(\s\w*https)?www.*$/,'');
      record.payee = record.payee.replace(/\s*\w+\.com\s+\w{2}$/, '');
      record.note  = record.note.replace( /\s*\w+\.com\s+\w{2}$/, '');
      record.payee = record.payee.replace(/aws.amazon.cWA/i,''); // serviaws.amazon.cWA
      record.note  = record.note.replace(/aws.amazon.cWA/i,'');
      if (record.payee.match(/(bostype \/ wes bo)(hamilton\s+on)/)) { // WES BOHAMILTON    ON
          record.payee = 'Wes Bos';
          record.note  = record.note.replace(RegExp.$1,'Wes Bos');
          record.note  = record.note.replace(RegExp.$2, '');
      }
      record.payee = record.payee.replace(/\s{2,}/g,' ');
      record.note  = record.note.replace(/\s{2,}/g,' ');

      /*
        'DEBIT PURCHASE -VISA SQ *PHIL        877-417-4551WA'

        You paid Phil $159 for Atreus keyboard kit and shipping

        It is for a credit card processor that goes by the brand name
        Square Up. Merchants can run credit card transactions through
        their iPhone or iPads using the Square Up services. Mine was for
        a taxi ride. https://800notes.com/Phone.aspx/1-877-417-4551
      ,*/

      record.payee = record.payee.replace(/sq/, 'square');
      record.note  = record.note.replace(/sq/, 'square');

      return record;
  }
#+end_src

#+RESULTS: stream-transform-function
: undefined

*** Set Up the CSV File Transform

#+caption:Create the CSV File Transform
#+name:create-csv-file-transform
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/csv/file_transformer.js
  /* csv/file_transformer */

  console.log('Entered file_transformer');

  const transform_fn = require('./transform_function');
  const transformer  = require('csv').transform(transform_fn);

  module.exports = function (options) {

      const stringifier   = require('./file_stringifier')(options.csv);
      const db            = require('../sql/db').attach(options.attach);
      const db_data       = require('../sql/db_data');

      /* TRANSFORMER reads records through its TRANSFORM_FUNCTION */
      /* -------------------------------------------------------- */
      transformer.on('readable', function() { // the rabbit does its thing
          let record;
          while ((record = transformer.read())) { // and spits it out
              console.log(`Transformer record:\n${util.inspect(record)}`);

              /* STRINGIFIER WRITE Records */
              /* ------------------------- */
              stringifier.write(record); // and continue down the rabbit hole



              /* DB RUN---INSERT RECORDS */
              /* ----------------------- */
              const tab_name  = db_data.DB_TABLES['usb'];
              const col_names = db_data.DB_COLS.join(',');
              const col_phs   = db_data.DB_COLS.map(c => '?').join(',');
              const col_values= db_data.DB_COLS.map(c => record[c]);

              let sql = `INSERT INTO ${ tab_name }( ${ col_names } )
                     VALUES ( ${ col_phs } )`;

              console.log(`sql: ${ sql }`);
              console.log(`col_values: ${ col_values }`);

              db.run(sql, col_values, (err) => { /* WRONG DB */
                  if (err) {
                      console.error(err.message);
                      console.error(`ERROR sql: ${ sql }`);
                      console.error(`ERROR values: ${ col_values }`);
                      process.exit(1);
                  }
              });
          }
      });

      transformer.on('error', function(err) {
          console.error(err.message);

      }).on('finish', function() {
          console.log('Transformer finished writing records.');

      }).on('end', function() {
          console.log('Transformer end reached.');
          stringifier.end();

      });

  }
#+end_src

** Set Up CSV-Stringify
#+cindex:@file{csv-stringify}
This section receives the transformed records from the Transform function and
writes them to new CSV files.  The new CSV files will be located in the
=WORKCSV= directory, i.e., ~WORKFIN/csv~.  A file will be called, for example,
~usb_6815__2016.csv~.  Notice that this file name uses two underscores, whereas
the source files use two dashes; in all other respects, they are the same.

#+caption:Create the CSV File Stringifier
#+name:create-csv-file-stringifier
#+header: :mkdirp yes
#+begin_src js -n :tangle lib/csv/file_stringifier.js
  /* csv/file_stringifier */

  console.log('Entered file_stringifier.js');

  const fs   = require('fs');
  const path = require('path');
  const {WORKCSV} = require('../../config/env');
  const {DB_COLS} = require('../sql/db_data');

  const file_stringifier = require('csv').stringify({
      header: true,
      columns: DB_COLS,
  });

  module.exports = function([acct, year]) {

      /* TODO: verify acct, year */

      const usb_acct      = `usb_${acct}`;
      const usb_acct_year = path.format({
          name: `${usb_acct}__${year}`,
          ext: '.csv'
      });

      const csv_path_file = path.join(
          WORKCSV,
          usb_acct_year
      );

      let csv_file_stream;
      try {
          csv_file_stream = fs.createWriteStream(csv_path_file);
          console.log(`WRITE STREAM: ${csv_path_file} has been successfully opened.`);

          csv_file_stream.on('close', function() {
              console.log('csv_file_stream is now closed');
          });
      } catch (err) {
          console.error(err.message);
          process.exit(1);
      }

      file_stringifier.on('readable', function() {
          console.log('file_stringifier is now readable');
          let row;
          while (row = this.read()) {
              console.log(`stringifer row: ${row}`);
              csv_file_stream.write(row);
          }

      }).on('error', function(err) {
          console.error(err.message);

      }).on('finish', function() {
          console.log('file_stringifier is done writing to csv_stringifer');
          csv_file_stream.end('stringifer called csv_file_stream\'s "end" method');

      }).on('close', function() {
          console.log('file_stringifier is now closed');

      });
  }
#+end_src

* Export SQLite DB Data to CSV File
Once data has been downloaded from the bank's web site and imported into the
sqlite3 database, it must be converted into a Ledger file.  This is
accomplished using the {{{option(--export)}}} option.

** =--export= Option

#+cindex:@option{--export} option
To export the SQLite3 database data to CSV and Ledger files, use the
{{{option(--export)}}} option:

: --export ACCT YEAR [--attach DB]

The csv filename is optional, with the default being the same as the db file
(i.e., ~workfin~), with the extension ~.csv~, (i.e., ~workfin.csv~) in the
~WORKCSV~ directory.  In order to facilitate matching an account to reconcile
against, the ~--export~ option must be accompanied by the designation of a bank
account, e.g., =6815= and a year, e.g., =2016=.  These will filter the data
that is exported from the ~sqlite~ data file and will become the account to
reconcile against.

The export is accomplished by executing in a child process the command line
program:

: sqlite3 -header -csv db sql-statements

The child process runs the {{{command(sqlite3)}}} command, and connects the
=STDOUT= stream to the target CSV file.  Since the entire database contents for
a given year and account are exported, the output will truncate the target CSV
file upon opening it for writing.  The program will halt after the export.

*** Export Code

#+caption:CSV-SQLite3 Export Option
#+name:csv-sqlite3-export-option
#+begin_src js -n :tangle lib/sql/export.js
  /* lib/sql/export.js */

  const { spawnSync } = require('child_process');
  const fs      = require('fs');
  const path    = require('path');
  const ENV     = require('../../config/env');
  const {WORKCSV,WORKLEDGER} = ENV.WORKCSV;
  const DB_DATA = require('./db_data');

  /*--EXPORT--*/
  //if (options.hasOwnProperty('export')) {

  module.exports.export = function (options) {

      const db_file = options.attach; //full path, e.g. WORKDB/workfin.sqlite
      const export_csv = path.basename(db_file, '.sqlite'); //e.g. workfin

      const export_csv_dir = WORKCSV;
      if (!fs.existsSync(export_csv_dir)) {
          fs.mkdirSync(export_csv_dir);
          console.log(`Created ${export_csv_dir}`);
      }
      const export_csv_path = path.format({
          dir: export_csv_dir,
          name: export_csv,
          ext: '.csv'
      });

      const [_acct, _year] = options.export;

      if (!(Object.keys(DB_DATA.DB_ACCTS).includes(_acct) &&
            DB_DATA.DB_YEARS.includes(_year))) {
          console.error(`Invalid values for acct: ${_acct} or year: ${_year}`);
          process.exit(1);
      }

      const usb_acct = `usb_${_acct}`;

      //'as' - Open file for appending in synchronous mode. The file is created if it does not exist.
      let fd = fs.openSync(export_csv_path,'as');
      const size = fs.statSync(export_csv_path).size;
      const header = size === 0 ? 'header' : 'noheader';
      console.log(`export_csv_path: ${export_csv_path}`);

      const sql = `
  SELECT ${DB_DATA.EXPORT_DB_COLS.join(',')}
  FROM   usb
  WHERE  acct = '${usb_acct}' and date like '${_year}%';`;

      console.log(`sql: ${sql}`);

      let ret = spawnSync(
          'sqlite3',
          [
              db_path,
              '-csv',
              `-${header}`,
              sql,
          ],
          {
              encoding: 'utf-8',
              stdio: [0,fd,2]
          }
      );

      if (ret.error) {
          console.log(`status: ${ret.status}\tsignal: ${ret.signal}`);
          console.log(`error: ${ret.error}`);
      }

      console.log('done exporting');
      fs.closeSync(fd);


      /* CONVERT CSV TO LEDGER */
      const ledger_dir = WORKLEDGER;
      const ledger_path = path.format({
          dir: ledger_dir,
          name: export_csv,
          ext: '.exported.ledger'
      });
      const zero_file = path.format({
          dir: ledger_dir,
          name: 'zero',
          ext: '.ledger'
      });
      if (!fs.existsSync(ledger_dir)) {
          fs.mkdirSync(ledger_dir);
      }

      const l_file = zero_file;
      console.log(`converting: ${export_csv_path} to ledger_path: ${ledger_path}`);

      fd = fs.openSync(ledger_path, 'as');	// 'as' - Open file for appending in synchronous mode.
                                                  // The file is created if it does not exist.
      ret = spawnSync(
          'ledger',
          [
              'convert',
              `${export_csv_path}`,
              '--invert',
              '--input-date-format=%Y-%m-%d',
              `--account=Assets:${DB_ACCTS[_acct]}`,
              '--rich-data',
              `--file=${l_file}`,
              `--now=${(new Date()).toISOString().split('T')[0]}`,
          ],
          {
              encoding: 'utf-8',
              stdio: [0,fd,2],
          }
      );

      if (ret.error) {
          console.log(`status: ${ret.status}\tsignal: ${ret.signal}`);
          console.log(`error: ${ret.error}`);
      }

      fs.closeSync(fd);
      process.exit(0);
  }
#+end_src

** Ledger ~convert~ Command

#+cindex:@command{convert} command
Upon an export of the SQLite3 data to a CSV file, the program will also send
the exported data through the Ledger {{{command(convert)}}} command and into
the ~workfin.ledger~ data file.[fn::Refer to the Ledger manual at Sec. 7.2.1.2
for the {{{command(convert}}}} command.]  This file is located in the ~ledger/~
directory below the ~workfin~ directory.

#+cindex:directives, Ledger
The {{{command(convert)}}} command uses a ~ledger~ file filled with ~ledger~
=directives= to associate =payee= 's with =account= 's.  If this =directives=
file does not exist, then it will be created.

** The Zero Ledger File
#+cindex:@command{convert} command
#+cindex:opening entry
#+cindex:accounts
The Zero Ledger File is a ~ledger~ file with an opening balance, list of
accounts, and directives that associate =payee= 's with =account= 's.  It is
used by the ~ledger~ {{{command(convert)}}} command to prepare a ~ledger~ file,
create initial set of accounts, and parse a CSV file into the ~ledger~ format.

{{{heading(List of Accounts)}}}

1. Expenses---where money goes

2. Assets---where money sits

3. Income---where money comes from

4. Liabilities---where money is owed

5. Equity---where value is


Beneath theses top levels, there can be any level of detail required.

{{{heading(Allowable Accounts)}}}

Here are defined some allowable accounts:

#+name:create-zero-file
#+header: :mkdirp yes
#+begin_src js :tangle ../../workfin/ledger/zero.ledger
account Expenses
account Assets
account Income
account Liabilities
account Equity

#+end_src

Use the {{{option(--strict)}}} option to show incampatible accounts

{{{heading(Opening Balances)}}}

The first entry is a set of opening balances.  It will look like this:

#+name:create-zero-file
#+begin_src js
2016/01/01 * Opening Balance
    Expenses				$0
	Assets:USB:Personal 6151		$0
	Assets:USB:Business 6815		$0
    Assets:USB:Trust 6831			$0
    Assets:USB:Savings			$0
    Income					$0
    Liabilities				$0
	Equity:Opening Balance

#+end_src

{{{heading(Directives and Subdirectives)}}}

The Zero file uses two directives, each of which uses a sub-directive, of the
form:

#+begin_example
payee <PAYEE>
  alias </PAYEE_REGEX/>
account <FULL:ACCOUNT>
  payee </PAYEE_REGEX/>
#+end_example

#+cindex:directives
#+cindex:@code{payee}
#+cindex:@code{account}
In the above, the first line rewrites the =payee= field to establish a
legitimate payee.  The =alias= is a regex; anything that matches this directive
will be turned into the associated =payee=.  The second line uses an account
and a =payee= directive to specify the proper =account=.  Anything that matches
the =payee= regex will be assigned the account.

#+name:create-zero-file
#+begin_src js
payee USPS
 alias usps
account Expenses:Office:Postage
 payee ^(USPS)$

payee Staples
 alias staples
payee Ikea
 alias ikea
payee Portland Art Museum
 alias portland art
payee The Energy Bar
 alias energy bar
account Expenses:Office:Supplies
 payee ^(Staples|Portland Art Museum|Ikea|The Energy Bar)$

payee City of Portland
 alias city of portland
account Expenses:Business:Travel
 payee ^(City of Portland)$

payee RingCentral
 alias ringcentral
payee AT&T
 alias (at&?t)
payee CenturyLink
 alias (centurylink|ctl)
payee NameBright
 alias namebright
account Expenses:Business:Communication
 payee ^(RingCentral|AT\&T|CenturyLink|NameBright)$

payee AVVO
 alias avvo
account Expenses:Business:Advertising
 payee AVVO

payee National Law Foundation
 alias national law fou
payee Coursera
 alias coursera
payee EdX Inc.
 alias edx
account Expenses:Professional:CLE
 payee ^(National Law Foundation|Coursera|EdX Inc.)$

payee State Bar of CA
 alias state bar of ca
account Expenses:Professional:License
 payee ^(State Bar of CA)$

payee Costco Gas
 alias costco gas
account Expenses:Office:Transportation
 payee ^(Costco Gas)$

payee American Express
 alias amex
payee Citi
 alias citi
account CC:Payment
 payee ^(American Express|Citi)$

payee Apple Store
 alias apple
payee Radio Shack
 alias radioshack
account Expenses:Office:Supplies
 payee ^(Apple Store|Radio Shack)$

payee State Bar WA
 alias interest paid this period
account Trust:LTAB
 payee State Bar WA

payee State Bar WA
 alias ltab
account Trust:LTAB
 payee State Bar WA

account Assets:Personal
 payee usb_6151

#+end_src

{{{heading(An =include= File }}}

Finally, include a file with an =Accounts:Payable= Category:

#+name:create-zero-file
#+begin_src js
include accounts_payable.ledger
#+end_src

** The Accounts Payable File

The ~accounts_payable.ledger~ file contains any outstanding accounts that
should be included to make the inputted data correct, such as a set of
outstanding invoices:

#+name:accounts_payable-file
#+begin_src js :tangle ../../workfin/ledger/accounts_payable.ledger
  2016/01/18 * Clark County Indigent Defense ; Invoice No.s 092--099
          Assets:Accounts Receivable	$1852.50
          Income:120703			-$  82.50 ; Invoice No 092
          Income:140707			-$ 525.00 ; Invoice No 093
          Income:140709			-$ 397.50 ; Invoice No 094
          Income:150701			-$  15.00 ; Invoice No 095
          Income:150704			-$ 742.50 ; Invoice No 096
          Income:150705			-$   7.50 ; Invoice No 097
          Income:150706			-$   7.50 ; Invoice No 098
          Income:150707			-$  75.00 ; Invoice No.099
#+end_src

* Find and Store Checks
:PROPERTIES:
:appendix: t
:END:

After the Sqlite3 database is exported and converted by Ledger, there are
numerous individual entries that need to be converted but for which there is no
real data available to help.  An example would be checks.  The check
information is located in the worklog, so one solution is to parse the worklog
to obtain check information, then parse the Ledger file to update it.

I created a package called =wlparser= that can be used to find particular
elements in the ~worklog.<year>.otl~ files.  Among other things, this package
is set up to find and return all checks for a particular year.  At this point,
the ~findChecks~ program requires an argument for a =<year>=, and will then
gather all checks and place them into the SQLite3 database in a table called
=checks=.  It can be run any number of times for any yeaer and will not add
duplicate entries.

When this ~csv-sqlite3~ program converts the CSV files into the =ledger=
format, it can do a search for a particular check number in this SQLite3
=checks= table and incorporate that data into the ledger file.

Use the commandline program ~find-checks <year>~ to find and store checks from
the worklog yearly file identified by =<year>= option (i.e.,
~worklog.<year>.otl~) and save it into the SQLite3 database =<db>= in a table
called =checks=.

#+caption:The =find-checks= script file
#+name:find-checks-script-file
#+header: :mkdirp yes :noweb yes
#+header: :shebang "#!/usr/bin/env node"
#+begin_src js -n :tangle scripts/find-checks.js
  /* find-checks.js */

  /* USAGE:
   ,* find-checks <year> [db]
   ,*/

  const DEFAULT_DB = 'workfin';
  const TABLE      = 'checks';
  const EXT        = '.sqlite';

  // make sure WORKDB is defined
  if (typeof process.env.WORKDB === 'undefined') {
    console.error('Must defined environment variable for WORKDB');
    process.exit(1);
  }
  const WORKDB = process.env.WORKDB;

  // make sure a <year> argument is included
  if (process.argv.length < 3) {
    console.error('Must include a <year> argument: "find-checks <year>"');
    process.exit(1);
  }

  // make sure the <year> argument is a number
  const wlyear = parseInt(process.argv[2],10);
  if (isNaN(wlyear)) {
    console.error(`The <year> argument: "${process.argv[2]}" must be a year, e.g., "2016"`);
    process.exit(1);
  }

  // second optional argument is the name of the database, without extension
  // if no second argument, use default db of $WORKDB/workfin.sqlite
  const path   = require('path');
  const db_path = path.format({
    dir: WORKDB,
    name: `${process.argv[3] || DEFAULT_DB}`,
    ext: EXT
  });

  // Everything is a go; load the wlparser, wlchecks, sqlite3 modules
  const {WLChecks}   = require('wlparser');
  const wlchecks     = new WLChecks(wlyear);
  const sqlite3      = require('sqlite3').verbose(); // remove verbose() for production code
  const {CHECK_COLS} = require('../lib/sql/definitions');

  let statement;

  // Load the sqlite3 database
  const db = new sqlite3.Database(db_path, err => {
    if (err) {
      console.error(`Database Error: ${err}`);
      process.exit(1);
    }
    console.log(`Successfully opened database at ${db_path}`);
  });

  db.serialize();

  statement = `CREATE TABLE IF NOT EXISTS
  <<define-checks-table-schema>>`;
  db.run(statement);

  let cols = CHECKS_COLS.join(','); // create string of column names for INSERT statement
  let values = CHECKS_COLS.map(col => `$${col}`).join(', '); // create string of placeholders for INSERT statement
  statement = `INSERT INTO ${TABLE} (${cols}) VALUES (${values})`;

  let all_checks = []; // used to filter out already-entered checks

  wlchecks.on('check', data => {
    delete data.type; // simply don't need this property
    if (!all_checks.includes(data.checkno)) { // filter out already-entered checks
        const new_data = {};
        for (k in data) { // create the named parameters of form 'new_data = {$checkno: 1234}'
            new_data[`$${k}`] = data[k];
        }
        db.run(statement, new_data, (err) => { // add the check data to the sqlite database
            if (err) console.error(`ERROR: ${err}`);
        });
    };

  }).on('checked', () => {
    db.close();

  }).on('error', err => {
    process.exit(1);

  });

  // load all of the previously-entered checks into the array 'all_checks'
  db.all(`SELECT checkno FROM ${TABLE} WHERE date LIKE '${wlyear}%'`, [], (err, check_data) => {
      if (check_data) {
          all_checks = check_data.map(row => row.checkno);
      }

      wlchecks.findChecks(); // start the stream running
  });

#+end_src

#+RESULTS: find-checks-script-file

{{{heading(Create Symbolic Link into WORKBIN)}}}

This code creates a symbolic link into the =WORKBIN= directory for the command
line program =find-checks= so that it can be run as a script.  Note that the
link must be symbolic in order for the ~node_modules/wlparser/lib/*~ files to
be found when ~find-checks~ is run from the =WORKBIN= directory.

#+begin_src sh :results output :exports both :dir scripts
ln -sf $PWD/find-checks.js $WORKBIN/find-checks
#+end_src

* Converting CSV Files
:PROPERTIES:
:appendix: t
:END:

** Ledger Convert Command
- Ledger manual sec. 7.2.1.2


#+cindex:Ledger @command{convert} command
#+cindex:@command{convert} command, Ledger
#+cindex:csv file, parse
#+cindex:parse csv file
The {{{command(convert)}}} command parses a comma separated value (csv) file
and prints Ledger transactions.  Importing csv files is a lot of work, but is
very amenable to scripting.

: $ ledger convert download.csv --input-date-format "%m/%d/%Y"

{{{heading(Fields Descriptors)}}}

#+cindex:fields, csv file
#+cindex:csv file fields
Your banks csv files will have fields in different orders from other banks, so
there must be a way to tell Ledger what to expect.  Ledger expects the first
line to contain a description of the fields on each line of the file.

- Insert a line at the beginning of the csv file that describes the fields to
  Ledger.

- The fields ledger can recognize contain these case-insensitive strings:

  #+attr_texinfo: :indic code
  - date

  - posted

  - code

  - payee

  - desc / description

  - amount

  - cost

  - total

  - note

- the =Input Date Format= option tells ledger how to interpret the dates:

  - =--input-date-format DATE_FORMAT=

  - e.g., ="%m/%d/%Y"=

  #+cindex:metadata
- Metadata

  - If there are columns in the bank data you would like to keep in your ledger
    data, besides the primary fields described above, you can name them in the
    field descriptor list and Ledger will include them in the transaction as
    meta data if it doesnt recognize the field name.

  - CSV :: original line from the csv file

  - Imported :: date data imported

  - UUID :: unique checksum; if an entry with the same UUID tag is already
            included in the normal ledger file (specified via {{{option(--file
            FILE (-f))}}} or via the environment variable =LEDGER_FILE=) this
            entry will not be printed again.


{{{heading(Convert Command Options)}}}

The convert command accepts the following options:

#+attr_texinfo: :indic option

#+cindex:@option{invert} option
#+cindex:convert option, @option{invert}
- --invert :: inverts the amount field

              #+cindex:@option{auto-match} option
              #+cindex:convert option, @option{--auto-match}
- --auto-match :: automatically matches an account from the Ledger journal for
                  every CSV line

                  #+cindex:@option{account} option
                  #+cindex:convert option, @option{--account}
- --account STR :: use to specify the account to balance against

                   #+cindex:@option{rich-data} option
                   #+cindex:convert option, @option{--rich-data}
- --rich-data :: stores additional tag/value pairs

                 #+cindex:environment variable @var{LEDGER_FILE}
                 #+cindex:@var{LEDGER_FILE} environment variable
                 #+cindex:@option{--file} option
                 #+cindex:convert option, @option{--file}
- --file (-f) :: the normal ledger file (could also be specified via the
                 environment variable =LEDGER_FILE=)

                 #+cindex:@option{--input-data-format} option
                 #+cindex:convert option, @option{--input-data-format}
- --input-date-format :: tells ledger how to interpret the dates

{{{heading(Command Directives)}}}

#+cindex:directives
#+cindex:command directives
#+cindex:@code{payee} directive
#+cindex:@code{alias} directive
#+cindex:@code{account} directive
You can also use {{{command(convert)}}} with =payee= and =account= directives.

1. Directives

   a. =directive= :: Command directives must occur at the beginning of a line.

   b. =account= directive :: Pre-declare valid account names.  This only has an
        effect if {{{option(--strict)}}} or {{{option(--pedantic)}}} is used.
        The =account= directive supports several optional sub-directives, if
        they immediately follow the =account== directive and if they begin with
        whitespace:

        #+begin_example
        account Expenses:Food
                      note This account is all about the chicken!
                      alias food
                      payee ^(KFC|Popeyes)$
                      check commodity == "$"
                      assert commodity == "$"
                      eval print("Hello!")
                      default
        #+end_example

   c. =payee= sub-directive :: The =payee= sub-directive of the =account=
        directive, which can occur multiple times, provides regexes that
        identify the =account= if that payee is encountered and an =account=
        within its transaction ends in the name "Unknown".

        #+begin_example
        2012-02-27 KFC
                      Expenses:Unknown      $10.00  ; Read now as "Expenses:Food"
                      Assets:Cash
        #+end_example

   d. =alias= sub-directive :: The =alias= sub-directive of the =acount=
        directive, which can occur multiple times, allows the alias to be used
        in place of the full account name anywhere that account names are
        allowed.

   e. =payee= directive :: The payee directive supports two optional
        sub-directives, if they immediately follow the payee directive and--if
        it is on a successive line--begins with white-space:

        #+begin_example
        payee KFC
          alias KENTUCKY FRIED CHICKEN
          uuid 2a2e21d434356f886c84371eebac6e44f1337fda
        #+end_example

        The =alias= sub-directive of the =payee= directive provides a regex
        which, if it matches a parsed payee, the declared payee name is
        substituted:

   f. =alias= directive :: Define an alias for an account name.  The aliases
        are only in effect for transactions read in after the alias is defined
        and are affected by =account= directives that precede them.  The
        =alias= sub-directive, which can occur multiple times, allows the alias
        to be used in place of the full account name anywhere that account
        names are allowed.

      #+begin_example
      alias Dining=Expenses:Entertainment:Dining
      alias Checking=Assets:Credit Union:Joint Checking Account

      2011/11/28 YummyPalace
          Dining        $10.00
          Checking
      #+end_example

2. First, you can use the =payee= directive and its =alias= sub-directive to
   rewrite the =payee= field based on some rules.

   #+begin_example
   payee Aldi
         alias ^ALDI SUED SAGT DANKE
   #+end_example

3. Then you can use the =account= directive and its =payee= sub-directive to
   specify the account.

   #+begin_example
   account Aufwand:Einkauf:Lebensmittel
          payee ^(Aldi|Alnatura|Kaufland|REWE)$
   #+end_example


{{{subheading(Directive Example)}}}

#+begin_example
payee Aldi
          alias ^ALDI SUED SAGT DANKE
account Aufwand:Einkauf:Lebensmittel
          payee ^(Aldi|Alnatura|Kaufland|REWE)$
#+end_example

Note that it may be necessary for the output of {{{command(ledger convert)}}}
to be passed through {{{command(ledger print)}}} a second time if you want to
match on the new =payee= field.  During the {{{command(ledger convert)}}} run,
only the original =payee= name as specified in the csv data seems to be used.

* Update package.json Version
:PROPERTIES:
:appendix: t
:END:
In order to keep the ~package.json~ =version= number,
[[create-CSV-SQLite3-package][Create the CSV-SQLite3 Package]], in sync with this document's version number, I have
created a little script to update its version based upon the macro =version='s
current value.  This macro is defined at the very top of this Org source file
just below the title and date.  I update this version number after every
modification to this source file and before commiting the change.  This little
script will then be run whenever an installation occurs.

This script can be run by invoking the Makefile target =update-version=.  This
will checkout the =dev= branch, tangle the ~update-version.sh~ script into the
~scripts/~ directory, run it, amend the most recent commit to include the
updated version number, push the amended commit to Github, and finally create a
=prod= branch (for =production=), install all of the files and documentation,
and commit and push the =prod= branch to Github.  At this point, the package is
ready to be cloned from Github and contains the most recent version number, the
dependencies installed, and run.

This program is a little {{{command(sed))}}} script that modifies this Org
source file in-place (after creating and storing a backup of the source) by
copying the version number found in the macro and updating the version number
of the ~package.json~ file.  It runs very quickly.

#+caption:Update ~package.json~ Version Number
#+name:update-package.json-version-number
#+header: :mkdirp yes
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src sh :tangle scripts/update-version.sh
sed -i .bak -E -e '
/\#\+macro: version Version/bx
/,\/\\"version\\":/by
b
:x
h
s/^(.*macro: version Version )(.*)$/\2/
x
b
:y
H
x
s/\n//
s/^([[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+)(.*)([[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+)/\2\1/
' CSV-SQLite3.org
#+end_src

* Node-SQLite3 Module
:PROPERTIES:
:appendix: true
:END:
Asynchronous, non-blocking SQLite3 bindings for Node.js.

- [[https://github.com/mapbox/node-sqlite3][Github]]

- [[https://github.com/mapbox/node-sqlite3/wiki/API][Wiki API]]

** Node-SQLite3 Module Usage
#+name:node-sqlite3-module-sample-usage
#+begin_src js -n
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(':memory:');

db.serialize(function() {
  db.run("CREATE TABLE lorem (info TEXT)");

  var stmt = db.prepare("INSERT INTO lorem VALUES (?)");
  for (var i = 0; i < 10; i++) {
      stmt.run("Ipsum " + i);
  }
  stmt.finalize();

  db.each("SELECT rowid AS id, info FROM lorem", function(err, row) {
      console.log(row.id + ": " + row.info);
  });
});

db.close();
#+end_src

** Features
- Straightforward query and parameter binding interface
- Full Buffer/Blob support
- Extensive debugging support
  #+cindex:serialization
- Query serialization API
- Extension support
- Big test suite
- Written in modern C++ and tested for memory leaks
- Bundles Sqlite3 3.26.0 as a fallback if the installing system doesn't include
  SQLite

** Node-SQLite3 API
#+cindex:serialization, function call
~node-sqlite3~ has built-in /function call serialization/ and automatically waits
before executing a blocking action until no other action is pending.  This
means that it's safe to start calling functions on the database object even if
it is not yet fully opened.  The ~Database#close()~ function will wait until
all pending queries are completed before closing the database.

** Node-SQLite3 Control Flow---Two Execution Modes
#+cindex:execution flow
#+cindex:parallel execution
#+cindex:exclusive mode
~node-sqlite3~ provides two functions to help control the execution flow of
statements.  The default mode is to execute statements in /parallel/.  However,
the ~Database#close~ method will always run in /exclusive mode/, meaning it
waits until all previous queries have completed and ~node-sqlite3~ will not run
any other queries while a ~close~ is pending.

*** Serialize Execution Mode
#+cindex:execution mode, serialize
#+cindex:serialize execution mode
#+cindex:serialized mode

#+attr_texinfo: :options Database serialize ( [callback] )
#+begin_defmethod
Puts the /execution mode/ into /serialized mode/.  This means that at most one
statement object can execute a query at a time.  Other statements wait in a
queue until the previous statements are executed.

If a callback is provided, it will be called immediately.  All database queries
scheduled in that callback will be serialized.  After the function returns, the
database is set back to its original mode again.
#+end_defmethod

Calling ~Database#serialize()~ within nested functions is safe:

#+name:node-sqlite-3-serialized-mode-example
#+begin_src js -n
  // Queries scheduled here will run in parallel.

  db.serialize(function() {

      // Queries scheduled here will be serialized.
      db.serialize(function() {
          // Queries scheduled here will still be serialized.
      });
      // Queries scheduled here will still be serialized.
  });

  // Queries scheduled here will run in parallel again.

#+end_src

Note that queries scheduled not directly in the callback function are not
necessarily serialized:

#+begin_src js -n
  db.serialize(function() {

      // These two queries will run sequentially.
      db.run("CREATE TABLE foo (num)");
      db.run("INSERT INTO foo VALUES (?)", 1, function() {

          // These queries will run in parallel and the second query will probably
          // fail because the table might not exist yet.
          db.run("CREATE TABLE bar (num)");
          db.run("INSERT INTO bar VALUES (?)", 1);
      });
  });
#+end_src

#+cindex:sticky execution mode
#+cindex:execution mode, sticky
If you call it without a function parameter, the execution mode setting is
sticky and won't change until the next call to ~Database#parallelize~.

*** Parallelize Execution Mode
#+cindex:parallized exeuction mode
#+cindex:execution mode, parallelized

#+attr_texinfo: :options Database parallelize ( [callback] )
#+begin_defmethod
Puts the execution mode into parallelized.  This means that queries scheduled
will be run in parallel.

If a callback is provided, it will be called immediately.  All database queries
scheduled in that callback will run parallelized.  After the function returns,
the database is set back to its original mode again.
#+end_defmethod

Calling ~Database#parallelize()~ within nested functions is safe:

#+begin_src js -n
  db.serialize(function() {

      // Queries scheduled here will be serialized.
      db.parallelize(function() {

          // Queries scheduled here will run in parallel.
      });

      // Queries scheduled here will be serialized again.
  });
#+end_src

If you call it without a function parameter, the execution mode setting is
sticky and won't change until the next call to ~Database#serialize~.

* Makefile
#+name:Makefile
#+begin_src make :tangle Makefile
SOURCE = CSV-SQLite3
ORG    = $(SOURCE).org
TEXI   = $(SOURCE).texi
INFO   = $(SOURCE).info
PDF    = $(SOURCE).pdf
DOCS   = docs
LIB    = lib
CONFIG = config
SCRIPTS=scripts

.PHONY: clean clean-world clean-prod
.PHONY: tangle weave texi info pdf
.PHONY: install install-docs install-info install-pdf open-pdf docs-dir
.PHONY: update-dev update-prod checkout-dev checkout-prod
.PHONY: update-version tangle-update-version run-update-version

texi: $(TEXI)
$(TEXI): $(ORG)
	emacs -Q --batch $(ORG) \
	--eval '(setq org-export-use-babel nil)' \
	--eval '(org-texinfo-export-to-texinfo)'

tangle: $(ORG)
	emacs -Q --batch $(ORG) \
	--eval '(org-babel-tangle-file "$(ORG)")'

info weave install-info: $(DOCS)/$(INFO)
$(DOCS)/$(INFO): $(TEXI) | docs-dir
	makeinfo --output=$(DOCS)/ $(TEXI)

install: package.json
package.json:	$(ORG) | docs-dir clean
	emacs -Q --batch $(ORG) \
	--eval '(require '\''ob-shell)' \
	--eval '(require '\''ob-js)' \
	--eval '(setq org-confirm-babel-evaluate nil)' \
	--eval '(org-texinfo-export-to-info)'
	mv $(INFO) $(DOCS)/
	make install-pdf

install-docs: install-info install-pdf

pdf install-pdf: $(DOCS)/$(PDF)
$(DOCS)/$(PDF): $(TEXI) | docs-dir
	pdftexi2dvi -q -c $(TEXI)
	mv $(PDF) $(DOCS)/

open-pdf: $(DOCS)/$(PDF)
	open $(DOCS)/$(PDF)

docs-dir: $(DOCS)
$(DOCS):
	mkdir -vp docs


update-version: update-dev update-prod

checkout-dev:
	git checkout dev

update-dev: checkout-dev run-update-version
	git add -u
	git commit --amend -C HEAD
	git push origin +dev

checkout-prod: clean-world checkout-dev
	git checkout -B prod

update-prod: checkout-prod install clean-prod
	git add -A .
	git commit -m "Branch:prod"
	git push origin +prod

run-update-version: tangle-update-version
	./$(SCRIPTS)/update-version.sh
	mv -v $(ORG).bak $(WORKBAK)/$(ORG).$(shell date "+%s")

tangle-update-version: $(SCRIPTS)/update-version.sh
$(SCRIPTS)/update-version.sh: $(ORG)
	emacs -Q --batch $(ORG) \
	--eval '(search-forward ":tangle scripts/update-version.sh")' \
	--eval '(org-babel-tangle '\''(4))'


clean:
	-rm *~

clean-world: clean
	-rm *.{texi,info,pdf,js,json,lock,log,bak}
	-rm -rf LogReader
	-rm -rf node_modules $(SCRIPTS) $(DOCS) $(LIB) $(CONFIG)

clean-prod: clean
	-rm *.{texi,org} Makefile LogReader
	-rm -rf node_modules

#+end_src

* Index
:PROPERTIES:
:unnumbered: t
:index:    cp
:END:

* Function Index
:PROPERTIES:
:index:    fn
:unnumbered: true
:END:

* Listings
:PROPERTIES:
:unnumbered: t
:END:
#+texinfo:@listoffloats Listing
* Copying
:PROPERTIES:
:copying:  t
:END:
{{{title}}} {{{version}}} ({{{date(%a %02m-%02d-%Y)}}})

\copy {{{date(%Y)}}} {{{author}}}

* Macro Definitions                                                :noexport:
#+macro: heading @@texinfo:@heading @@$1
#+macro: subheading @@texinfo:@subheading @@$1
#+macro: noindent @@texinfo:@noindent @@
#+macro: option @@texinfo:@option{@@$1@@texinfo:}@@
#+macro: command @@texinfo:@command{@@$1@@texinfo:}@@

* Options                                                          :noexport:
#+startup: indent
#+options: H:4 ':t
* Export Settings                                                  :noexport:
#+texinfo_filename:CSV-SQLite3.info
#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:CSV
#+texinfo_dir_title:ConvertCSV (convertcsv)
#+texinfo_dir_desc:Convert USB CSV files to SQLite
#+texinfo_printed_title:ConvertCSV Using Node.js CSV-Parser

* Local Variables                                                  :noexport:
# Local Variables:
# time-stamp-pattern:"8/^\\#\\+date:<%:y-%02m-%02d %3a %02H:%02M>$"
# eval: (auto-fill-mode)
# eval: (column-number-mode)
# End:
